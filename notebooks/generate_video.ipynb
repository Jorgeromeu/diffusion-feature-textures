{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import text3d2video.wandb_util as wu\n",
    "from text3d2video.artifacts.vertex_atributes_artifact import VertAttributesArtifact\n",
    "from text3d2video.artifacts.animation_artifact import AnimationArtifact\n",
    "from text3d2video.ipython_utils import display_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3d_artifact_tag = \"deadpool-3d-features:v6\"\n",
    "animation_tag = \"backflip:latest\"\n",
    "\n",
    "features_3d = wu.get_artifact(features_3d_artifact_tag)\n",
    "features_3d = VertAttributesArtifact.from_wandb_artifact(features_3d)\n",
    "\n",
    "animation = wu.get_artifact(animation_tag)\n",
    "animation = AnimationArtifact.from_wandb_artifact(animation)\n",
    "\n",
    "mv_features = features_3d.get_mv_features_from_lineage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6489ae0473cc4ed5bb7476a43322c324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import ControlNetModel\n",
    "from text3d2video.cross_frame_attn import CrossFrameAttnProcessor\n",
    "from text3d2video.pipelines.my_pipeline import MyPipeline\n",
    "\n",
    "dtype = torch.float16\n",
    "sd_repo = \"runwayml/stable-diffusion-v1-5\"\n",
    "controlnet_repo = \"lllyasviel/sd-controlnet-depth\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_repo, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipe = MyPipeline.from_pretrained(sd_repo, controlnet=controlnet, torch_dtype=dtype).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from scripts.generate_video import render_feature_images, render_feature_images_batched\n",
    "from text3d2video.ipython_utils import display_frames_as_video\n",
    "from text3d2video.rendering import render_depth_map\n",
    "from text3d2video.util import front_camera\n",
    "\n",
    "frame_indices = [1, 2, 3]\n",
    "do_feature_injection = True\n",
    "num_steps = 30\n",
    "prompt = \"Deadpool\"\n",
    "seed = 1\n",
    "\n",
    "# render depth maps\n",
    "camera = front_camera()\n",
    "frames = animation.load_frames(frame_indices)\n",
    "depth_maps = render_depth_map(frames, camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = features_3d.get_features_disk_dict().key_values(\"layer\")\n",
    "timesteps = features_3d.get_features_disk_dict().key_values(\"timestep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch3d.structures.meshes.Meshes at 0x7dcaa4bcd760>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from text3d2video.rendering import rasterize_vertex_features\n",
    "\n",
    "camera = front_camera()\n",
    "frames = animation.load_frames(frame_indices)\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch3d.structures import join_meshes_as_batch\n",
    "\n",
    "frames_expanded = join_meshes_as_batch([frames] * 3)\n",
    "len(frames_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:01, 16.69it/s]\n"
     ]
    }
   ],
   "source": [
    "timesteps = list(range(30))\n",
    "\n",
    "# render feature images\n",
    "with torch.no_grad():\n",
    "\n",
    "    all_feature_images = render_feature_images(\n",
    "        features_3d,\n",
    "        mv_features,\n",
    "        animation,\n",
    "        frame_indices,\n",
    "        timesteps=timesteps,\n",
    "        layers=[layers[0]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:10<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Attention Processor setup\n",
    "attn_processor = CrossFrameAttnProcessor(unet_chunk_size=2, pipe=pipe)\n",
    "attn_processor.feature_images_multidict = all_feature_images\n",
    "attn_processor.do_cross_frame_attn = True\n",
    "attn_processor.do_feature_injection = True\n",
    "attn_processor.feature_blend_alpha = 1.0\n",
    "pipe.unet.set_attn_processor(attn_processor)\n",
    "\n",
    "# run pipeline\n",
    "prompts = [prompt] * len(frame_indices)\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "generator.manual_seed(seed)\n",
    "images = pipe(prompts, depth_maps, generator=generator, num_inference_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video outs/vid.mp4.\n",
      "Moviepy - Writing video outs/vid.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready outs/vid.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"/home/jorge/thesis/outs/vid.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_frames_as_video(images, Path(\"outs/vid.mp4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
