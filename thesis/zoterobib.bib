@online{friedmanLearningTransformerPrograms2023,
  title = {Learning {{Transformer Programs}}},
  author = {Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  date = {2023-10-30},
  eprint = {2306.01128},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.01128},
  urldate = {2024-03-23},
  abstract = {Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the "circuits" used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {RASP},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:26:32.745Z},
  file = {/home/jorge/Zotero/storage/LDBGN627/Friedman et al. - 2023 - Learning Transformer Programs.pdf}
}

@online{vit,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2024-03-23},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,ViT},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:26:29.819Z},
  file = {/home/jorge/Zotero/storage/54VR58R4/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-03-23},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:36:35.250Z},
  file = {/home/jorge/Zotero/storage/ZDEUB2SP/Vaswani et al. - 2023 - Attention Is All You Need.pdf}
}

@online{weissThinkingTransformers2021,
  title = {Thinking {{Like Transformers}}},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  date = {2021-07-19},
  eprint = {2106.06981},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.06981},
  urldate = {2024-03-23},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder‚Äîattention and feed-forward computation‚Äîinto simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {RASP},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:33:00.076Z},
  file = {/home/jorge/Zotero/storage/G3T9HUHY/Weiss et al. - 2021 - Thinking Like Transformers.pdf}
}

@online{SpatialTransformers,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  date = {2016-02-04},
  eprint = {1506.02025},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02025},
  urldate = {2024-03-24},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:32:05.007Z},
  file = {/home/jorge/Zotero/storage/RM99YMIT/Jaderberg et al. - 2016 - Spatial Transformer Networks.pdf}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2024-03-24},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-19T12:44:03.802Z},
  file = {/home/jorge/Zotero/storage/APSBJYX8/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@online{shawSelfAttentionRelativePosition2018,
  title = {Self-{{Attention}} with {{Relative Position Representations}}},
  author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  date = {2018-04-12},
  eprint = {1803.02155},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.02155},
  url = {http://arxiv.org/abs/1803.02155},
  urldate = {2024-03-24},
  abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
  pubstate = {prepublished},
  keywords = {üî•,Transformers},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-04T12:42:01.644Z},
  file = {/home/jorge/Zotero/storage/HFV2YXGS/Shaw et al. - 2018 - Self-Attention with Relative Position Representations.pdf;/home/jorge/Zotero/storage/BJX2FTUK/1803.html}
}

@online{jangCategoricalReparameterizationGumbelSoftmax2017,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  date = {2017-08-05},
  eprint = {1611.01144},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.01144},
  urldate = {2024-03-26},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:32:25.765Z},
  file = {/home/jorge/Zotero/storage/PMHQJ8S4/1611.01144.pdf}
}

@online{maddisonConcreteDistributionContinuous2017,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  shorttitle = {The {{Concrete Distribution}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  date = {2017-03-05},
  eprint = {1611.00712},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.00712},
  urldate = {2024-03-26},
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables‚ÄîCONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:36:21.905Z},
  file = {/home/jorge/Zotero/storage/WFFYEQDL/Maddison et al. - 2017 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables.pdf}
}

@online{cordonnierRelationshipSelfAttentionConvolutional2020,
  title = {On the {{Relationship}} between {{Self-Attention}} and {{Convolutional Layers}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  date = {2020-01-10},
  eprint = {1911.03584},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1911.03584},
  urldate = {2024-03-26},
  abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {ViT},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-04T12:41:22.766Z},
  file = {/home/jorge/Zotero/storage/F48EVDP3/Cordonnier et al. - 2020 - On the Relationship between Self-Attention and Convolutional Layers.pdf}
}

@online{ciampiDeepLearningTechniques2022,
  title = {Deep {{Learning Techniques}} for {{Visual Counting}}},
  author = {Ciampi, Luca},
  date = {2022-06-08},
  eprint = {2206.03033},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.03033},
  urldate = {2024-03-27},
  abstract = {In this dissertation, we investigated and enhanced Deep Learning (DL) techniques for counting objects, like pedestrians, cells or vehicles, in still images or video frames. In particular, we tackled the challenge related to the lack of data needed for training current DL-based solutions. Given that the budget for labeling is limited, data scarcity still represents an open problem that prevents the scalability of existing solutions based on the supervised learning of neural networks and that is responsible for a significant drop in performance at inference time when new scenarios are presented to these algorithms. We introduced solutions addressing this issue from several complementary sides, collecting datasets gathered from virtual environments automatically labeled, proposing Domain Adaptation strategies aiming at mitigating the domain gap existing between the training and test data distributions, and presenting a counting strategy in a weakly labeled data scenario, i.e., in the presence of non-negligible disagreement between multiple annotators. Moreover, we tackled the non-trivial engineering challenges coming out of the adoption of Convolutional Neural Network-based techniques in environments with limited power resources, introducing solutions for counting vehicles and pedestrians directly onboard embedded vision systems, i.e., devices equipped with constrained computational capabilities that can capture images and elaborate them.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-04T12:26:19.430Z},
  file = {/home/jorge/Zotero/storage/E7G67XLH/Ciampi - 2022 - Deep Learning Techniques for Visual Counting.pdf}
}

@online{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-03-02},
  eprint = {1502.03167},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2024-04-02},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer‚Äôs inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T11:28:49.070Z},
  file = {/home/jorge/Zotero/storage/RB5UTRLB/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf}
}

@online{wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  date = {2018-06-11},
  eprint = {1803.08494},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.08494},
  urldate = {2024-04-02},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems ‚Äî BN‚Äôs error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN‚Äôs usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN‚Äôs computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BNbased counterparts for object detection and segmentation in COCO,1 and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-25T08:49:48.686Z},
  file = {/home/jorge/Zotero/storage/URFG2PBV/Wu and He - 2018 - Group Normalization.pdf}
}

@online{DETR,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  date = {2020-05-28},
  eprint = {2005.12872},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.12872},
  urldate = {2024-04-04},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading,Object Detection,ViT},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-07T09:29:36.334Z},
  file = {/home/jorge/Zotero/storage/FFLSYFLW/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf}
}

@online{NeRF,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  eprint = {2003.08934},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2024-04-04},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (Œ∏, œÜ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,cv-seminar-reading,NeRF,Neural Fields},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-04T12:45:23.297Z},
  file = {/home/jorge/Zotero/storage/PT67KN9T/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Fields for View Synthesis.pdf}
}

@online{beyerBetterPlainViT2022,
  title = {Better Plain {{ViT}} Baselines for {{ImageNet-1k}}},
  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  date = {2022-05-03},
  eprint = {2205.01580},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.01580},
  urldate = {2024-04-04},
  abstract = {It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76\% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80\% in less than one day.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-04T14:24:27.848Z},
  file = {/home/jorge/Zotero/storage/8KHWLVM9/Beyer et al. - 2022 - Better plain ViT baselines for ImageNet-1k.pdf}
}

@article{lecunGradientBasedLearning,
  title = {Gradient {{Based Learning Applied}} to {{Document Recognition}}},
  author = {LeCun, Yann},
  langid = {english},
  keywords = {üî•},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-19T12:44:37.057Z},
  file = {/home/jorge/Zotero/storage/HBDVYIQG/LeCun - GradientBased Learning Applied to Document Recognition.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2024-04-05},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ‚Äúdropout‚Äù that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-05T12:54:31.974Z},
  file = {/home/jorge/Zotero/storage/A4TAJXVY/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf}
}

@online{jaquesPhysicsasInverseGraphicsUnsupervisedPhysical2020,
  title = {Physics-as-{{Inverse-Graphics}}: {{Unsupervised Physical Parameter Estimation}} from {{Video}}},
  shorttitle = {Physics-as-{{Inverse-Graphics}}},
  author = {Jaques, Miguel and Burke, Michael and Hospedales, Timothy},
  date = {2020-04-21},
  eprint = {1905.11169},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.11169},
  urldate = {2024-04-15},
  abstract = {We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as-inverse-graphics and differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller‚Äôs interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-15T22:33:17.717Z},
  file = {/home/jorge/Zotero/storage/PWKYWZUE/Jaques et al. - 2020 - Physics-as-Inverse-Graphics Unsupervised Physical Parameter Estimation from Video.pdf}
}

@online{NeuRBF,
  title = {{{NeuRBF}}: {{A Neural Fields Representation}} with {{Adaptive Radial Basis Functions}}},
  shorttitle = {{{NeuRBF}}},
  author = {Chen, Zhang and Li, Zhong and Song, Liangchen and Chen, Lele and Yu, Jingyi and Yuan, Junsong and Xu, Yi},
  date = {2023-09-27},
  eprint = {2309.15426},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.15426},
  urldate = {2024-04-15},
  abstract = {We present a novel type of neural fields that uses general radial bases for signal representation. State-of-the-art neural fields typically rely on grid-based representations for storing local neural features and N-dimensional linear kernels for interpolating features at continuous query points. The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals. Our method instead builds upon general radial bases with flexible kernel position and shape, which have higher spatial adaptivity and can more closely fit target signals. To further improve the channel-wise capacity of radial basis functions, we propose to compose them with multi-frequency sinusoid functions. This technique extends a radial basis to multiple Fourier radial bases of different frequency bands without requiring extra parameters, facilitating the representation of details. Moreover, by marrying adaptive radial bases with grid-based ones, our hybrid combination inherits both adaptivity and interpolation smoothness. We carefully designed weighting schemes to let radial bases adapt to different types of signals effectively. Our experiments on 2D image and 3D signed distance field representation demonstrate the higher accuracy and compactness of our method than prior arts. When applied to neural radiance field reconstruction, our method achieves state-of-the-art rendering quality, with small model size and comparable training speed.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-15T22:38:11.343Z},
  file = {/home/jorge/Zotero/storage/87LJ2NRK/Chen et al. - 2023 - NeuRBF A Neural Fields Representation with Adaptive Radial Basis Functions.pdf}
}

@online{kayhanTranslationInvarianceCNNs2020,
  title = {On {{Translation Invariance}} in {{CNNs}}: {{Convolutional Layers}} Can {{Exploit Absolute Spatial Location}}},
  shorttitle = {On {{Translation Invariance}} in {{CNNs}}},
  author = {Kayhan, Osman Semih and family=Gemert, given=Jan C., prefix=van, useprefix=true},
  date = {2020-05-30},
  eprint = {2003.07064},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2003.07064},
  urldate = {2024-04-18},
  abstract = {In this paper we challenge the common assumption that convolutional layers in modern CNNs are translation invariant. We show that CNNs can and will exploit the absolute spatial location by learning filters that respond exclusively to particular absolute locations by exploiting image boundary effects. Because modern CNNs filters have a huge receptive field, these boundary effects operate even far from the image boundary, allowing the network to exploit absolute spatial location all over the image. We give a simple solution to remove spatial location encoding which improves translation invariance and thus gives a stronger visual inductive bias which particularly benefits small data sets. We broadly demonstrate these benefits on several architectures and various applications such as image classification, patch matching, and two video classification datasets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-29T11:33:53.296Z},
  file = {/home/jorge/Zotero/storage/DLMBKBK9/Kayhan and van Gemert - 2020 - On Translation Invariance in CNNs Convolutional Layers can Exploit Absolute Spatial Location.pdf}
}

@online{UNet,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  eprint = {1505.04597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2024-04-18},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,cv-seminar-reading},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-29T11:33:57.791Z},
  file = {/home/jorge/Zotero/storage/3W62V9SE/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf}
}

@online{mostafaSupervisedLearningBased2017,
  title = {Supervised Learning Based on Temporal Coding in Spiking Neural Networks},
  author = {Mostafa, Hesham},
  date = {2017-08-16},
  eprint = {1606.08165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.08165},
  urldate = {2024-04-18},
  abstract = {Gradient descent training techniques are remarkably successful in training analogvalued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard nonlinearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is piece-wise linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior can not be directly approximated by conventional ANNs. Our results highlight a new approach for controlling the behavior of spiking networks with realistic temporal dynamics, opening up the potential for using these networks to process spike patterns with complex temporal information.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-04-23T17:47:23.557Z},
  file = {/home/jorge/Zotero/storage/TFSSM45D/Mostafa - 2017 - Supervised learning based on temporal coding in spiking neural networks.pdf}
}

@online{MAE,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll√°r, Piotr and Girshick, Ross},
  date = {2021-12-19},
  eprint = {2111.06377},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.06377},
  urldate = {2024-04-18},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3√ó or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-23T17:47:25.172Z},
  file = {/home/jorge/Zotero/storage/BPB5LGKB/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf}
}

@online{jiaVisualPromptTuning2022,
  title = {Visual {{Prompt Tuning}}},
  author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  date = {2022-07-20},
  eprint = {2203.12119},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.12119},
  urldate = {2024-04-18},
  abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-23T17:47:31.179Z},
  file = {/home/jorge/Zotero/storage/XJZKR6MH/Jia et al. - 2022 - Visual Prompt Tuning.pdf}
}

@online{PointNet,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  date = {2017-04-10},
  eprint = {1612.00593},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.00593},
  urldate = {2024-04-18},
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-23T17:47:13.907Z},
  file = {/home/jorge/Zotero/storage/X7FTMC8E/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf}
}

@online{musgraveMetricLearningReality2020,
  title = {A {{Metric Learning Reality Check}}},
  author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
  date = {2020-09-15},
  eprint = {2003.08505},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.08505},
  urldate = {2024-04-18},
  abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best. Code is available at github.com/KevinMusgrave/powerful-benchmarker.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-08-18T17:24:17.826Z},
  file = {/home/jorge/Zotero/storage/UVCUGIY4/Musgrave et al. - 2020 - A Metric Learning Reality Check.pdf}
}

@online{carreiraQuoVadisAction2018,
  title = {Quo {{Vadis}}, {{Action Recognition}}? {{A New Model}} and the {{Kinetics Dataset}}},
  shorttitle = {Quo {{Vadis}}, {{Action Recognition}}?},
  author = {Carreira, Joao and Zisserman, Andrew},
  date = {2018-02-12},
  eprint = {1705.07750},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.07750},
  urldate = {2024-04-18},
  abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {cv-seminar-reading},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-29T11:34:02.896Z},
  file = {/home/jorge/Zotero/storage/GDAXPB3Y/Carreira and Zisserman - 2018 - Quo Vadis, Action Recognition A New Model and the Kinetics Dataset.pdf}
}

@online{SIREN,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  date = {2020-06-17},
  eprint = {2006.09661},
  eprinttype = {arXiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2006.09661},
  urldate = {2024-04-19},
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal‚Äôs spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Neural Fields},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-26T12:58:06.074Z},
  file = {/home/jorge/Zotero/storage/ATEBLYCN/Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Activation Functions.pdf}
}

@online{dupontDataFunctaYour2022,
  title = {From Data to Functa: {{Your}} Data Point Is a Function and You Can Treat It like One},
  shorttitle = {From Data to Functa},
  author = {Dupont, Emilien and Kim, Hyunjik and Eslami, S. M. Ali and Rezende, Danilo and Rosenbaum, Dan},
  date = {2022-11-10},
  eprint = {2201.12204},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.12204},
  urldate = {2024-04-19},
  abstract = {It is common practice in deep learning to represent a measurement of the world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying signal represented by these measurements is often continuous, e.g. the scene depicted in an image. A powerful continuous alternative is then to represent these measurements using an implicit neural representation, a neural function trained to output the appropriate measurement value for any input spatial location. In this paper, we take this idea to its next level: what would it take to perform deep learning on these functions instead, treating them as data? In this context we refer to the data as functa, and propose a framework for deep learning on functa. This view presents a number of challenges around efficient conversion from data to functa, compact representation of functa, and effectively solving downstream tasks on functa. We outline a recipe to overcome these challenges and apply it to a wide range of data modalities including images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We demonstrate that this approach has various compelling properties across data modalities, in particular on the canonical tasks of generative modeling, data imputation, novel view synthesis and classification. Code: github.com/deepmind/functa.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Neural Fields},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-29T11:30:18.960Z},
  file = {/home/jorge/Zotero/storage/FCAGN2TP/Dupont et al. - 2022 - From data to functa Your data point is a function and you can treat it like one.pdf}
}

@online{ResNet,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2024-04-20},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers‚Äî8√ó deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-20T12:51:09.749Z},
  file = {/home/jorge/Zotero/storage/6UIIFU6Z/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@online{zhangMakingConvolutionalNetworks2019,
  title = {Making {{Convolutional Networks Shift-Invariant Again}}},
  author = {Zhang, Richard},
  date = {2019-06-08},
  eprint = {1904.11486},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.11486},
  urldate = {2024-04-20},
  abstract = {Modern convolutional networks are not shiftinvariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and averagepooling, ignore the sampling theorem. The wellknown signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe increased accuracy in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe better generalization, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-20T13:19:15.928Z},
  file = {/home/jorge/Zotero/storage/RC44PN6G/Zhang - 2019 - Making Convolutional Networks Shift-Invariant Again.pdf}
}

@online{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  date = {2018-01-11},
  eprint = {1603.07285},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1603.07285},
  urldate = {2024-04-20},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-20T20:23:00.593Z},
  file = {/home/jorge/Zotero/storage/YD68K9NB/Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learning.pdf}
}

@online{xieNeuralFieldsVisual2022,
  title = {Neural {{Fields}} in {{Visual Computing}} and {{Beyond}}},
  author = {Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},
  date = {2022-04-05},
  eprint = {2111.11426},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.11426},
  urldate = {2024-04-22},
  abstract = {Recent advances in machine learning have led to increased interest in solving visual computing problems using methods that employ coordinate-based neural networks. These methods, which we call neural fields, parameterize physical properties of scenes or objects across space and time. They have seen widespread success in problems such as 3D shape and image synthesis, animation of human bodies, 3D reconstruction, and pose estimation. Rapid progress has led to numerous papers, but a consolidation of the discovered knowledge has not yet emerged. We provide context, mathematical grounding, and a review of over 250 papers in the literature on neural fields. In Part I, we focus on neural field techniques by identifying common components of neural field methods, including different conditioning, representation, forward map, architecture, and manipulation methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, and highlights the improved quality, flexibility, and capability brought by neural field methods. Finally, we present a companion website that acts as a living database that can be continually updated by the community.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Neural Fields},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-04-24T12:33:23.082Z},
  file = {/home/jorge/Zotero/storage/S7FRBT6K/Xie et al. - 2022 - Neural Fields in Visual Computing and Beyond.pdf}
}

@inproceedings{tancikLearnedInitializationsOptimizing2021,
  title = {Learned {{Initializations}} for {{Optimizing Coordinate-Based Neural Representations}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tancik, Matthew and Mildenhall, Ben and Wang, Terrance and Schmidt, Divi and Srinivasan, Pratul P. and Barron, Jonathan T. and Ng, Ren},
  date = {2021-06},
  pages = {2845--2854},
  publisher = {IEEE},
  location = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.00287},
  url = {https://ieeexplore.ieee.org/document/9578751/},
  urldate = {2024-04-24},
  abstract = {Coordinate-based neural representations have shown significant promise as an alternative to discrete, arraybased representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  keywords = {Meta Learning,NeRF,Neural Fields},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-19T18:53:38.491Z},
  file = {/home/jorge/Zotero/storage/B4GBMC7P/Tancik et al. - 2021 - Learned Initializations for Optimizing Coordinate-Based Neural Representations.pdf}
}

@online{MAML,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2024-04-24},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Meta Learning},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-02T13:20:36.325Z},
  file = {/home/jorge/Zotero/storage/HP6H5FTN/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf}
}

@online{kayKineticsHumanAction2017,
  title = {The {{Kinetics Human Action Video Dataset}}},
  author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
  date = {2017-05-19},
  eprint = {1705.06950},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.06950},
  urldate = {2024-04-29},
  abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-29T09:28:55.276Z},
  file = {/home/jorge/Zotero/storage/PRSTBTYU/Kay et al. - 2017 - The Kinetics Human Action Video Dataset.pdf}
}

@online{xuSignalProcessingImplicit2022,
  title = {Signal {{Processing}} for {{Implicit Neural Representations}}},
  author = {Xu, Dejia and Wang, Peihao and Jiang, Yifan and Fan, Zhiwen and Wang, Zhangyang},
  date = {2022-12-15},
  eprint = {2210.08772},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.08772},
  urldate = {2024-04-29},
  abstract = {Implicit Neural Representations (INRs) encoding continuous multi-media data via multi-layer perceptrons has shown undebatable promise in various computer vision tasks. Despite many successful applications, editing and processing an INR remains intractable as signals are represented by latent parameters of a neural network. Existing works manipulate such continuous representations via processing on their discretized instance, which breaks down the compactness and continuous nature of INR. In this work, we present a pilot study on the question: how to directly modify an INR without explicit decoding? We answer this question by proposing an implicit neural signal processing network, dubbed INSP-Net, via differential operators on INR. Our key insight is that spatial gradients of neural networks can be computed analytically and are invariant to translation, while mathematically we show that any continuous convolution filter can be uniformly approximated by a linear combination of high-order differential operators. With these two knobs, INSP-Net instantiates the signal processing operator as a weighted composition of computational graphs corresponding to the high-order derivatives of INRs, where the weighting parameters can be data-driven learned. Based on our proposed INSPNet, we further build the first Convolutional Neural Network (CNN) that implicitly runs on INRs, named INSP-ConvNet. Our experiments validate the expressiveness of INSP-Net and INSP-ConvNet in fitting low-level image and geometry processing kernels (e.g. blurring, deblurring, denoising, inpainting, and smoothening) as well as for high-level tasks on implicit fields such as image classification.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Neural Fields},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-29T11:24:00.540Z},
  file = {/home/jorge/Zotero/storage/UUGUDNNU/Xu et al. - 2022 - Signal Processing for Implicit Neural Representations.pdf}
}

@online{kofinasGraphNeuralNetworks2024,
  title = {Graph {{Neural Networks}} for {{Learning Equivariant Representations}} of {{Neural Networks}}},
  author = {Kofinas, Miltiadis and Knyazev, Boris and Zhang, Yan and Chen, Yunlu and Burghouts, Gertjan J. and Gavves, Efstratios and Snoek, Cees G. M. and Zhang, David W.},
  date = {2024-03-20},
  eprint = {2403.12143},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2403.12143},
  urldate = {2024-04-29},
  abstract = {Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to learn from neural graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Neural Fields},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-29T11:26:27.876Z},
  file = {/home/jorge/Zotero/storage/89FQU2KQ/Kofinas et al. - 2024 - Graph Neural Networks for Learning Equivariant Representations of Neural Networks.pdf}
}

@online{tancikFourierFeaturesLet2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  date = {2020-06-18},
  eprint = {2006.10739},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.10739},
  urldate = {2024-04-29},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in lowdimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF,Neural Fields},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-06T15:42:29.517Z},
  file = {/home/jorge/Zotero/storage/9P75442L/Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains.pdf}
}

@online{bauerSpatialFunctaScaling2023,
  title = {Spatial {{Functa}}: {{Scaling Functa}} to {{ImageNet Classification}} and {{Generation}}},
  shorttitle = {Spatial {{Functa}}},
  author = {Bauer, Matthias and Dupont, Emilien and Brock, Andy and Rosenbaum, Dan and Schwarz, Jonathan Richard and Kim, Hyunjik},
  date = {2023-02-09},
  eprint = {2302.03130},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.03130},
  urldate = {2024-04-29},
  abstract = {Neural fields, also known as implicit neural representations, have emerged as a powerful means to represent complex signals of various modalities. Based on this Dupont et al. (2022a) introduce a framework that views neural fields as data, termed functa, and proposes to do deep learning directly on this dataset of neural fields. In this work, we show that the proposed framework faces limitations when scaling up to even moderately complex datasets such as CIFAR-10. We then propose spatial functa, which overcome these limitations by using spatially arranged latent representations of neural fields, thereby allowing us to scale up the approach to ImageNet-1k at 256 √ó 256 resolution. We demonstrate competitive performance to Vision Transformers (Steiner et al., 2022) on classification and Latent Diffusion (Rombach et al., 2022) on image generation respectively.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Neural Fields},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-04-29T11:30:06.307Z},
  file = {/home/jorge/Zotero/storage/QCQ87YCL/Bauer et al. - 2023 - Spatial Functa Scaling Functa to ImageNet Classification and Generation.pdf}
}

@online{Faster-R-CNN,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2016-01-06},
  eprint = {1506.01497},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.01497},
  urldate = {2024-04-30},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features‚Äîusing the recently popular terminology of neural networks with ‚Äúattention‚Äù mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Object Detection},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-26T14:07:18.826Z},
  file = {/home/jorge/Zotero/storage/NIA7GPZD/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf}
}

@online{deluigiDeepLearningImplicit2023,
  title = {Deep {{Learning}} on {{Implicit Neural Representations}} of {{Shapes}}},
  author = {De Luigi, Luca and Cardace, Adriano and Spezialetti, Riccardo and Ramirez, Pierluigi Zama and Salti, Samuele and Di Stefano, Luigi},
  date = {2023-02-10},
  eprint = {2302.05438},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.05438},
  urldate = {2024-05-02},
  abstract = {Implicit Neural Representations (INRs) have emerged in the last few years as a powerful tool to encode continuously a variety of different signals like images, videos, audio and 3D shapes. When applied to 3D shapes, INRs allow to overcome the fragmentation and shortcomings of the popular discrete representations used so far. Yet, considering that INRs consist in neural networks, it is not clear whether and how it may be possible to feed them into deep learning pipelines aimed at solving a downstream task. In this paper, we put forward this research problem and propose inr2vec, a framework that can compute a compact latent representation for an input INR in a single inference pass. We verify that inr2vec can embed effectively the 3D shapes represented by the input INRs and show how the produced embeddings can be fed into deep learning pipelines to solve several tasks by processing exclusively INRs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Neural Fields},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-02T09:09:02.758Z},
  file = {/home/jorge/Zotero/storage/48UYH8T5/De Luigi et al. - 2023 - Deep Learning on Implicit Neural Representations of Shapes.pdf}
}

@online{Reptile,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  date = {2018-10-22},
  eprint = {1803.02999},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.02999},
  urldate = {2024-05-02},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  pubstate = {prepublished},
  keywords = {üî•,Meta Learning},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-02T13:19:17.290Z},
  file = {/home/jorge/Zotero/storage/R2A563HY/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf}
}

@online{linBARFBundleAdjustingNeural2021,
  title = {{{BARF}}: {{Bundle-Adjusting Neural Radiance Fields}}},
  shorttitle = {{{BARF}}},
  author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
  date = {2021-08-19},
  eprint = {2104.06405},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.06405},
  urldate = {2024-05-05},
  abstract = {Neural Radiance Fields (NeRF) [31] have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of realworld scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses ‚Äî the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na√Øvely applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-05-06T15:42:24.983Z},
  file = {/home/jorge/Zotero/storage/BSBWZLSP/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf}
}

@article{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
  date = {2023-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {42},
  number = {4},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3592433},
  url = {https://dl.acm.org/doi/10.1145/3592433},
  urldate = {2024-05-06},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (‚â• 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  langid = {english},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-06T10:10:57.319Z},
  file = {/home/jorge/Zotero/storage/EUKLJ2MB/Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf}
}

@online{NeRF-W,
  title = {{{NeRF}} in the {{Wild}}: {{Neural Radiance Fields}} for {{Unconstrained Photo Collections}}},
  shorttitle = {{{NeRF}} in the {{Wild}}},
  author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
  date = {2021-01-06},
  eprint = {2008.02268},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2008.02268},
  urldate = {2024-05-06},
  abstract = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-19T18:53:42.096Z},
  file = {/home/jorge/Zotero/storage/2FJ2NDNT/Martin-Brualla et al. - 2021 - NeRF in the Wild Neural Radiance Fields for Unconstrained Photo Collections.pdf}
}

@online{srinivasanNeRVNeuralReflectance2020,
  title = {{{NeRV}}: {{Neural Reflectance}} and {{Visibility Fields}} for {{Relighting}} and {{View Synthesis}}},
  shorttitle = {{{NeRV}}},
  author = {Srinivasan, Pratul P. and Deng, Boyang and Zhang, Xiuming and Tancik, Matthew and Mildenhall, Ben and Barron, Jonathan T.},
  date = {2020-12-07},
  eprint = {2012.03927},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.03927},
  urldate = {2024-05-06},
  abstract = {We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model‚Äôs ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-19T18:53:48.319Z},
  file = {/home/jorge/Zotero/storage/VZE34QM2/Srinivasan et al. - 2020 - NeRV Neural Reflectance and Visibility Fields for Relighting and View Synthesis.pdf}
}

@online{uzolasTemplatefreeArticulatedNeural2023,
  title = {Template-Free {{Articulated Neural Point Clouds}} for {{Reposable View Synthesis}}},
  author = {Uzolas, Lukas and Eisemann, Elmar and Kellnhofer, Petr},
  date = {2023-10-31},
  eprint = {2305.19065},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.19065},
  urldate = {2024-05-06},
  abstract = {Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates. The project website can be found at https://lukas.uzolas.com/Articulated-Point-NeRF/.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-05-20T14:38:08.866Z},
  file = {/home/jorge/Zotero/storage/CJZLZ2SQ/Uzolas et al. - 2023 - Template-free Articulated Neural Point Clouds for Reposable View Synthesis.pdf}
}

@article{tumanyanDINOTrackerTamingDINO,
  title = {{{DINO-Tracker}}: {{Taming DINO}} for {{Self-Supervised Point Tracking}} in a {{Single Video}}},
  author = {Tumanyan, Narek and Singer, Assaf and Bagon, Shai and Dekel, Tali},
  abstract = {We present DINO-Tracker ‚Äì a new framework for long-term dense tracking in video. The pillar of our approach is combining test-time training on a single video, with the powerful localized semantic features learned by a pre-trained DINO-ViT model. Specifically, our framework simultaneously adopts DINO‚Äôs features to fit to the motion observations of the test video, while training a tracker that directly leverages the refined features. The entire framework is trained end-to-end using a combination of self-supervised losses, and regularization that allows us to retain and benefit from DINO‚Äôs semantic prior. Extensive evaluation demonstrates that our method achieves state-of-the-art results on known benchmarks. DINO-tracker significantly outperforms self-supervised methods and is competitive with state-of-the-art supervised trackers, while outperforming them in challenging cases of tracking under long-term occlusions.},
  langid = {english},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-16T18:25:34.735Z},
  file = {/home/jorge/Zotero/storage/NVLR9QRY/Tumanyan et al. - DINO-Tracker Taming DINO for Self-Supervised Point Tracking in a Single Video.pdf}
}

@online{D-NeRF,
  title = {D-{{NeRF}}: {{Neural Radiance Fields}} for {{Dynamic Scenes}}},
  shorttitle = {D-{{NeRF}}},
  author = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
  date = {2020-11-27},
  eprint = {2011.13961},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.13961},
  urldate = {2024-05-06},
  abstract = {Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \textbackslash emph\{single\} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-08-18T17:24:29.803Z},
  file = {/home/jorge/Zotero/storage/QRVHPK35/Pumarola et al. - 2020 - D-NeRF Neural Radiance Fields for Dynamic Scenes.pdf}
}

@online{yaoLASSIELearningArticulated2022,
  title = {{{LASSIE}}: {{Learning Articulated Shapes}} from {{Sparse Image Ensemble}} via {{3D Part Discovery}}},
  shorttitle = {{{LASSIE}}},
  author = {Yao, Chun-Han and Hung, Wei-Chih and Li, Yuanzhen and Rubinstein, Michael and Yang, Ming-Hsuan and Jampani, Varun},
  date = {2022-07-07},
  eprint = {2207.03434},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.03434},
  urldate = {2024-05-07},
  abstract = {Creating high-quality articulated 3D models of animals is challenging either via manual creation or using 3D scanning tools. Therefore, techniques to reconstruct articulated 3D objects from 2D images are crucial and highly useful. In this work, we propose a practical problem setting to estimate 3D pose and shape of animals given only a few (10-30) in-the-wild images of a particular animal species (say, horse). Contrary to existing works that rely on pre-defined template shapes, we do not assume any form of 2D or 3D ground-truth annotations, nor do we leverage any multi-view or temporal information. Moreover, each input image ensemble can contain animal instances with varying poses, backgrounds, illuminations, and textures. Our key insight is that 3D parts have much simpler shape compared to the overall animal and that they are robust w.r.t. animal pose articulations. Following these insights, we propose LASSIE, a novel optimization framework which discovers 3D parts in a self-supervised manner with minimal user intervention. A key driving force behind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory deep features. Experiments on Pascal-Part and self-collected in-the-wild animal datasets demonstrate considerably better 3D reconstructions as well as both 2D and 3D part discovery compared to prior arts. Project page: chhankyao.github.io/lassie/},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-07T09:39:29.912Z},
  file = {/home/jorge/Zotero/storage/NHGUQRVB/Yao et al. - 2022 - LASSIE Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery.pdf}
}

@online{DINO,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J√©gou, Herv√© and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  eprint = {2104.14294},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2024-05-07},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [19] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [33], multi-crop training [10], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {ViT},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-16T18:25:38.573Z},
  file = {/home/jorge/Zotero/storage/JTQQA23W/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf}
}

@online{NeRS,
  title = {{{NeRS}}: {{Neural Reflectance Surfaces}} for {{Sparse-view 3D Reconstruction}} in the {{Wild}}},
  shorttitle = {{{NeRS}}},
  author = {Zhang, Jason Y. and Yang, Gengshan and Tulsiani, Shubham and Ramanan, Deva},
  date = {2021-10-18},
  eprint = {2110.07604},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2110.07604},
  urldate = {2024-05-07},
  abstract = {Recent history has seen a tremendous growth of work exploring implicit representations of geometry and radiance, popularized through Neural Radiance Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric representation of occupancy, allowing them to model diverse scene structure including translucent objects and atmospheric obscurants. But because the vast majority of real-world scenes are composed of well-defined surfaces, we introduce a surface analog of such implicit models called Neural Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions. Even more importantly, surface parameterizations allow NeRS to learn (neural) bidirectional surface reflectance functions (BRDFs) that factorize view-dependent appearance into environmental illumination, diffuse color (albedo), and specular ‚Äúshininess.‚Äù Finally, rather than illustrating our results on synthetic scenes or controlled in-the-lab capture, we assemble a novel dataset of multi-view images from online marketplaces for selling goods. Such ‚Äúin-the-wild‚Äù multi-view image sets pose a number of challenges, including a small number of views with unknown/rough camera estimates. We demonstrate that surface-based neural reconstructions enable learning from such data, outperforming volumetric neural rendering-based reconstructions. We hope that NeRS serves as a first step toward building scalable, high-quality libraries of real-world shape, materials, and illumination. The project page with code and video visualizations can be found at jasonyzhang.com/ners.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-18T17:27:28.413Z},
  file = {/home/jorge/Zotero/storage/T942AFJ6/Zhang et al. - 2021 - NeRS Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild.pdf}
}

@online{amirDeepViTFeatures2022,
  title = {Deep {{ViT Features}} as {{Dense Visual Descriptors}}},
  author = {Amir, Shir and Gandelsman, Yossi and Bagon, Shai and Dekel, Tali},
  date = {2022-10-15},
  eprint = {2112.05814},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.05814},
  urldate = {2024-05-10},
  abstract = {We study the use of deep features extracted from a pretrained Vision Transformer (ViT) as dense visual descriptors. We observe and empirically demonstrate that such features, when extracted from a self-supervised ViT model (DINO-ViT), exhibit several striking properties, including: (i) the features encode powerful, well-localized semantic information, at high spatial granularity, such as object parts; (ii) the encoded semantic information is shared across related, yet different object categories, and (iii) positional bias changes gradually throughout the layers. These properties allow us to design simple methods for a variety of applications, including co-segmentation, part co-segmentation and semantic correspondences. To distill the power of ViT features from convoluted design choices, we restrict ourselves to lightweight zero-shot methodologies (e.g., binning and clustering) applied directly to the features. Since our methods require no additional training nor data, they are readily applicable across a variety of domains. We show by extensive qualitative and quantitative evaluation that our simple methodologies achieve competitive results with recent state-of-the-art supervised methods, and outperform previous unsupervised methods by a large margin. Code is available in dino-vit-features.github.io.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-05-16T18:25:29.077Z},
  file = {/home/jorge/Zotero/storage/KJKJFVPE/Amir et al. - 2022 - Deep ViT Features as Dense Visual Descriptors.pdf}
}

@online{zhangAddingConditionalControl2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  date = {2023-11-26},
  eprint = {2302.05543},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.05543},
  urldate = {2024-05-11},
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained textto-image diffusion models. ControlNet locks the productionready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with ‚Äúzero convolutions‚Äù (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({$<$}50k) and large ({$>$}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Diffusion},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-08-18T17:27:20.639Z},
  file = {/home/jorge/Zotero/storage/XIQYMQEJ/Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffusion Models.pdf}
}

@online{Omnimotion,
  title = {Tracking {{Everything Everywhere All}} at {{Once}}},
  author = {Wang, Qianqian and Chang, Yen-Yu and Cai, Ruojin and Li, Zhengqi and Hariharan, Bharath and Holynski, Aleksander and Snavely, Noah},
  date = {2023-09-12},
  eprint = {2306.05422},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.05422},
  urldate = {2024-05-11},
  abstract = {We present a new test-time optimization method for estimating dense and long-range motion from a video sequence. Prior optical flow or particle video tracking algorithms typically operate within limited temporal windows, struggling to track through occlusions and maintain global consistency of estimated motion trajectories. We propose a complete and globally consistent motion representation, dubbed OmniMotion, that allows for accurate, full-length motion estimation of every pixel in a video. OmniMotion represents a video using a quasi-3D canonical volume and performs pixel-wise tracking via bijections between local and canonical space. This representation allows us to ensure global consistency, track through occlusions, and model any combination of camera and object motion. Extensive evaluations on the TAP-Vid benchmark and real-world footage show that our approach outperforms prior state-of-the-art methods by a large margin both quantitatively and qualitatively. See our project page for more results: omnimotion.github.io.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-11T16:33:24.042Z},
  file = {/home/jorge/Zotero/storage/ZEWI7W9E/Wang et al. - 2023 - Tracking Everything Everywhere All at Once.pdf}
}

@online{DINOv2,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timoth√©e and Moutakanni, Th√©o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv√© and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  date = {2024-02-02},
  eprint = {2304.07193},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.07193},
  urldate = {2024-05-11},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing generalpurpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2021) with 1B parameters and distill it into a series of smaller models that surpass the best available general-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {ViT},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-16T18:25:36.926Z},
  file = {/home/jorge/Zotero/storage/HYQQNUDN/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Supervision.pdf}
}

@online{liSelfsupervisedSingleview3D2020,
  title = {Self-Supervised {{Single-view 3D Reconstruction}} via {{Semantic Consistency}}},
  author = {Li, Xueting and Liu, Sifei and Kim, Kihwan and De Mello, Shalini and Jampani, Varun and Yang, Ming-Hsuan and Kautz, Jan},
  date = {2020-03-13},
  eprint = {2003.06473},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.06473},
  urldate = {2024-05-13},
  abstract = {We learn a self-supervised, single-view 3D reconstruction model that predicts the 3D mesh shape, texture and camera pose of a target object with a collection of 2D images and silhouettes. The proposed method does not necessitate 3D supervision, manually annotated keypoints, multi-view images of an object or a prior 3D template. The key insight of our work is that objects can be represented as a collection of deformable parts, and each part is semantically coherent across different instances of the same category (e.g., wings on birds and wheels on cars). Therefore, by leveraging self-supervisedly learned part segmentation of a large collection of category-specific images, we can effectively enforce semantic consistency between the reconstructed meshes and the original images. This significantly reduces ambiguities during joint prediction of shape and camera pose of an object, along with texture. To the best of our knowledge, we are the first to try and solve the single-view reconstruction problem without a category-specific template mesh or semantic keypoints. Thus our model can easily generalize to various object categories without such labels, e.g., horses, penguins, etc. Through a variety of experiments on several categories of deformable and rigid objects, we demonstrate that our unsupervised method performs comparably if not better than existing category-specific reconstruction methods learned with supervision.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-16T18:26:08.733Z},
  file = {/home/jorge/Zotero/storage/2N66NSE5/Li et al. - 2020 - Self-supervised Single-view 3D Reconstruction via Semantic Consistency.pdf}
}

@online{zhangTaleTwoFeatures2023,
  title = {A {{Tale}} of {{Two Features}}: {{Stable Diffusion Complements DINO}} for {{Zero-Shot Semantic Correspondence}}},
  shorttitle = {A {{Tale}} of {{Two Features}}},
  author = {Zhang, Junyi and Herrmann, Charles and Hur, Junhwa and Cabrera, Luisa Polania and Jampani, Varun and Sun, Deqing and Yang, Ming-Hsuan},
  date = {2023-11-28},
  eprint = {2305.15347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.15347},
  urldate = {2024-05-15},
  abstract = {Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across multiple, different images and objects. In this work, we exploit Stable Diffusion (SD) features for semantic and dense correspondence and discover that with simple postprocessing, SD features can perform quantitatively similar to SOTA representations. Interestingly, our analysis reveals that SD features have very different properties compared to existing representation learning features, such as the recently released DINOv2: while DINOv2 provides sparse but accurate matches, SD features provide high-quality spatial information but sometimes inaccurate semantic matches. We demonstrate that a simple fusion of the two features works surprisingly well, and a zero-shot evaluation via nearest neighbor search on the fused features provides a significant performance gain over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show that these correspondences enable high-quality object swapping without task-specific fine-tuning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-19T20:28:56.463Z},
  file = {/home/jorge/Zotero/storage/GC9PHQU5/Zhang et al. - 2023 - A Tale of Two Features Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence.pdf}
}

@inproceedings{yeFeatureNeRFLearningGeneralizable2023,
  title = {{{FeatureNeRF}}: {{Learning Generalizable NeRFs}} by {{Distilling Foundation Models}}},
  shorttitle = {{{FeatureNeRF}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Ye, Jianglong and Wang, Naiyan and Wang, Xiaolong},
  date = {2023-10-01},
  eprint = {2303.12786},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {8928--8939},
  doi = {10.1109/ICCV51070.2023.00823},
  url = {http://arxiv.org/abs/2303.12786},
  urldate = {2024-05-15},
  abstract = {Recent works on generalizable NeRFs have shown promising results on novel view synthesis from single or few images. However, such models have rarely been applied on other downstream tasks beyond synthesis such as semantic understanding and parsing. In this paper, we propose a novel framework named FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D pre-trained foundation models to 3D space via neural rendering, and then extract deep features for 3D query points from NeRF MLPs. Consequently, it allows to map 2D images to continuous 3D semantic feature volumes, which can be used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D semantic keypoint transfer and 2D/3D object part segmentation. Our extensive experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D semantic feature extractor. Our project page is available at https: //jianglongye.com/featurenerf/.},
  langid = {english},
  keywords = {NeRF,Vision Foundation Models},
  file = {/home/jorge/Zotero/storage/DS8GXWXN/Ye et al. - 2023 - FeatureNeRF Learning Generalizable NeRFs by Distilling Foundation Models.pdf}
}

@article{zhangTellingLeftRight,
  title = {Telling {{Left}} from {{Right}}: {{Identifying Geometry-Aware Semantic Correspondence}}},
  author = {Zhang, Junyi and Herrmann, Charles and Hur, Junhwa and Chen, Eric and Jampani, Varun and Sun, Deqing and Yang, Ming-Hsuan},
  abstract = {While pre-trained large-scale vision models have shown significant promise for semantic correspondence, their features often struggle to grasp the geometry and orientation of instances. This paper identifies the importance of being geometry-aware for semantic correspondence and reveals a limitation of the features of current foundation models under simple post-processing. We show that incorporating this information can markedly enhance semantic correspondence performance with simple but effective solutions in both zero-shot and supervised settings. We also construct a new challenging benchmark for semantic correspondence built from an existing animal pose estimation dataset, for both pre-training validating models. Our method achieves a PCK@0.10 score of 65.4 (zero-shot) and 85.6 (supervised) on the challenging SPair-71k dataset, surpassing the state of the art by 5.5p and 11.0p absolute gains, respectively. Our code and datasets are publicly available at: https: //telling-left-from-right.github.io.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-16T17:26:46.917Z},
  file = {/home/jorge/Zotero/storage/59INW4YG/Zhang et al. - Telling Left from Right Identifying Geometry-Aware Semantic Correspondence.pdf}
}

@online{Nerfies,
  title = {Nerfies: {{Deformable Neural Radiance Fields}}},
  shorttitle = {Nerfies},
  author = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B. and Seitz, Steven M. and Martin-Brualla, Ricardo},
  date = {2021-09-09},
  eprint = {2011.12948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.12948},
  urldate = {2024-05-16},
  abstract = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ‚Äúnerfies.‚Äù We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  file = {/home/jorge/Zotero/storage/PBLN66TQ/Park et al. - 2021 - Nerfies Deformable Neural Radiance Fields.pdf}
}

@online{HyperNeRF,
  title = {{{HyperNeRF}}: {{A Higher-Dimensional Representation}} for {{Topologically Varying Neural Radiance Fields}}},
  shorttitle = {{{HyperNeRF}}},
  author = {Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B. and Martin-Brualla, Ricardo and Seitz, Steven M.},
  date = {2021-09-10},
  eprint = {2106.13228},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.13228},
  urldate = {2024-05-16},
  abstract = {Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this "hyper-space". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between "moments", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1\% for interpolation and 8.6\% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at https://hypernerf.github.io.},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-05-20T14:38:29.999Z},
  file = {/home/jorge/Zotero/storage/WZ482XP9/Park et al. - 2021 - HyperNeRF A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields.pdf}
}

@online{yangEmerNeRFEmergentSpatialTemporal2023,
  title = {{{EmerNeRF}}: {{Emergent Spatial-Temporal Scene Decomposition}} via {{Self-Supervision}}},
  shorttitle = {{{EmerNeRF}}},
  author = {Yang, Jiawei and Ivanovic, Boris and Litany, Or and Weng, Xinshuo and Kim, Seung Wook and Li, Boyi and Che, Tong and Xu, Danfei and Fidler, Sanja and Pavone, Marco and Wang, Yue},
  date = {2023-11-03},
  eprint = {2311.02077},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.02077},
  urldate = {2024-05-16},
  abstract = {We present EmerNeRF, a simple yet powerful approach for learning spatial-temporal representations of dynamic driving scenes. Grounded in neural fields, EmerNeRF simultaneously captures scene geometry, appearance, motion, and semantics via self-bootstrapping. EmerNeRF hinges upon two core components: First, it stratifies scenes into static and dynamic fields. This decomposition emerges purely from self-supervision, enabling our model to learn from general, in-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field from the dynamic field and uses this flow field to further aggregate multi-frame features, amplifying the rendering precision of dynamic objects. Coupling these three fields (static, dynamic, and flow) enables EmerNeRF to represent highly-dynamic scenes self-sufficiently, without relying on ground truth object annotations or pre-trained models for dynamic object segmentation or optical flow estimation. Our method achieves state-of-the-art performance in sensor simulation, significantly outperforming previous methods when reconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In addition, to bolster EmerNeRF's semantic generalization, we lift 2D visual foundation model features into 4D space-time and address a general positional bias in modern Transformers, significantly boosting 3D perception performance (e.g., 37.50\% relative improvement in occupancy prediction accuracy on average). Finally, we construct a diverse and challenging 120-sequence dataset to benchmark neural fields under extreme and highly-dynamic settings.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/jorge/Zotero/storage/YA9MGTF7/Yang et al. - 2023 - EmerNeRF Emergent Spatial-Temporal Scene Decomposition via Self-Supervision.pdf}
}

@online{dengDepthsupervisedNeRFFewer2022,
  title = {Depth-Supervised {{NeRF}}: {{Fewer Views}} and {{Faster Training}} for {{Free}}},
  shorttitle = {Depth-Supervised {{NeRF}}},
  author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
  date = {2022-04-29},
  eprint = {2107.02791},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2107.02791},
  urldate = {2024-05-16},
  abstract = {A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as "free" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-19T12:43:50.508Z},
  file = {/home/jorge/Zotero/storage/8UMLEBCT/Deng et al. - 2022 - Depth-supervised NeRF Fewer Views and Faster Training for Free.pdf}
}

@online{pixelNeRF,
  title = {{{pixelNeRF}}: {{Neural Radiance Fields}} from {{One}} or {{Few Images}}},
  shorttitle = {{{pixelNeRF}}},
  author = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
  date = {2021-05-30},
  eprint = {2012.02190},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.02190},
  urldate = {2024-05-18},
  abstract = {We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-18T17:27:28.412Z},
  file = {/home/jorge/Zotero/storage/T2SK9DT2/Yu et al. - 2021 - pixelNeRF Neural Radiance Fields from One or Few Images.pdf}
}

@online{mip-NeRF360,
  title = {Mip-{{NeRF}} 360: {{Unbounded Anti-Aliased Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}} 360},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  date = {2022-03-25},
  eprint = {2111.12077},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.12077},
  urldate = {2024-05-19},
  abstract = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on ‚Äúunbounded‚Äù scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub ‚Äúmip-NeRF 360‚Äù as we target scenes in which the camera rotates 360 degrees around a point, reduces meansquared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-19T13:12:18.871Z},
  file = {/home/jorge/Zotero/storage/GEKRZPBB/Barron et al. - 2022 - Mip-NeRF 360 Unbounded Anti-Aliased Neural Radiance Fields.pdf}
}

@inproceedings{barronMipNeRFMultiscaleRepresentation2021,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  date = {2021-10},
  pages = {5835--5844},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00580},
  url = {https://ieeexplore.ieee.org/document/9710056/},
  urldate = {2024-05-19},
  abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call ‚Äúmip-NeRF‚Äù (`a la ‚Äúmipmap‚Äù), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF‚Äôs ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22√ó faster.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  keywords = {NeRF},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-05-19T20:44:54.046Z},
  file = {/home/jorge/Zotero/storage/JTFZ2VHV/Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Aliasing Neural Radiance Fields.pdf}
}

@online{Plenoxels,
  title = {Plenoxels: {{Radiance Fields}} without {{Neural Networks}}},
  shorttitle = {Plenoxels},
  author = {Yu, Alex and Fridovich-Keil, Sara and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  date = {2021-12-09},
  eprint = {2112.05131},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.05131},
  urldate = {2024-05-19},
  abstract = {We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-08-18T17:24:49.821Z},
  file = {/home/jorge/Zotero/storage/ABNJZGCX/Yu et al. - 2021 - Plenoxels Radiance Fields without Neural Networks.pdf}
}

@article{mullerInstantNeuralGraphics2022,
  title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author = {M√ºller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  date = {2022-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3528223.3530127},
  url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
  urldate = {2024-05-19},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920√ó1080.},
  langid = {english},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-19T13:22:18.890Z},
  file = {/home/jorge/Zotero/storage/SZFKJ5V9/M√ºller et al. - 2022 - Instant neural graphics primitives with a multiresolution hash encoding.pdf}
}

@online{zhiInPlaceSceneLabelling2021,
  title = {In-{{Place Scene Labelling}} and {{Understanding}} with {{Implicit Scene Representation}}},
  author = {Zhi, Shuaifeng and Laidlow, Tristan and Leutenegger, Stefan and Davison, Andrew J.},
  date = {2021-08-21},
  eprint = {2103.15875},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.15875},
  urldate = {2024-05-20},
  abstract = {Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-20T09:02:35.727Z},
  file = {/home/jorge/Zotero/storage/DDNBYWWA/Zhi et al. - 2021 - In-Place Scene Labelling and Understanding with Implicit Scene Representation.pdf}
}

@inproceedings{fuPanopticNeRF3Dto2D2022,
  title = {Panoptic {{NeRF}}: {{3D-to-2D Label Transfer}} for {{Panoptic Urban Scene Segmentation}}},
  shorttitle = {Panoptic {{NeRF}}},
  booktitle = {2022 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Fu, Xiao and Zhang, Shangzhan and Chen, Tianrun and Lu, Yichong and Zhu, Lanyun and Zhou, Xiaowei and Geiger, Andreas and Liao, Yiyi},
  date = {2022-09},
  pages = {1--11},
  publisher = {IEEE},
  location = {Prague, Czech Republic},
  doi = {10.1109/3DV57658.2022.00042},
  url = {https://ieeexplore.ieee.org/document/10044395/},
  urldate = {2024-05-20},
  abstract = {Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is laborintensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to2D label transfer method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multiview consistent by design. Experimental results show that Panoptic NeRF outperforms existing label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.},
  eventtitle = {2022 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  isbn = {978-1-66545-670-8},
  langid = {english},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-18T17:25:39.149Z},
  file = {/home/jorge/Zotero/storage/9Y58K368/Fu et al. - 2022 - Panoptic NeRF 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation.pdf}
}

@online{gaoNeRFNeuralRadiance2023,
  title = {{{NeRF}}: {{Neural Radiance Field}} in {{3D Vision}}, {{A Comprehensive Review}}},
  shorttitle = {{{NeRF}}},
  author = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Dening and Xu, Linlin and Li, Jonathan},
  date = {2023-11-30},
  eprint = {2210.00379},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.00379},
  urldate = {2024-05-20},
  abstract = {Neural Radiance Field (NeRF) has recently become a significant development in the field of Computer Vision, allowing for implicit, neural network-based scene representation and novel view synthesis. NeRF models have found diverse applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Due to the growing popularity of NeRF and its expanding research area, we present a comprehensive survey of NeRF papers from the past two years. Our survey is organized into architecture and application-based taxonomies and provides an introduction to the theory of NeRF and its training via differentiable volume rendering. We also present a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-20T11:23:24.122Z},
  file = {/home/jorge/Zotero/storage/T8QMJ9E8/Gao et al. - 2023 - NeRF Neural Radiance Field in 3D Vision, A Comprehensive Review.pdf}
}

@online{RefNeRF,
  title = {Ref-{{NeRF}}: {{Structured View-Dependent Appearance}} for {{Neural Radiance Fields}}},
  shorttitle = {Ref-{{NeRF}}},
  author = {Verbin, Dor and Hedman, Peter and Mildenhall, Ben and Zickler, Todd and Barron, Jonathan T. and Srinivasan, Pratul P.},
  date = {2021-12-07},
  eprint = {2112.03907},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.03907},
  urldate = {2024-05-20},
  abstract = {Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF‚Äôs parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model‚Äôs internal representation of outgoing radiance is interpretable and useful for scene editing.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-20T14:10:22.635Z},
  file = {/home/jorge/Zotero/storage/3I95A2QC/Verbin et al. - 2021 - Ref-NeRF Structured View-Dependent Appearance for Neural Radiance Fields.pdf}
}

@online{Point-NeRF,
  title = {Point-{{NeRF}}: {{Point-based Neural Radiance Fields}}},
  shorttitle = {Point-{{NeRF}}},
  author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
  date = {2023-03-15},
  eprint = {2201.08845},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.08845},
  urldate = {2024-05-20},
  abstract = {Volumetric neural rendering methods like NeRF generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30X faster training time. Point-NeRF can be combined with other 3D reconstruction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism. The experiments on the DTU, the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets demonstrate Point-NeRF can surpass the existing methods and achieve the state-of-the-art results.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-05-20T14:38:06.804Z},
  file = {/home/jorge/Zotero/storage/8TN29B8X/Xu et al. - 2023 - Point-NeRF Point-based Neural Radiance Fields.pdf}
}

@article{kobayashiDecomposingNeRFEditing2022,
  title = {Decomposing {{NeRF}} for {{Editing}} via {{Feature Field Distillation}}},
  author = {Kobayashi, Sosuke and Matsumoto, Eiichi and Sitzmann, Vincent},
  date = {2022},
  abstract = {Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, supervised and self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.},
  langid = {english},
  keywords = {NeRF,Vision Foundation Models},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-20T17:20:18.922Z},
  file = {/home/jorge/Zotero/storage/BSUNVDXV/Kobayashi et al. - Decomposing NeRF for Editing via Feature Field Distillation.pdf}
}

@online{KiloNeRF,
  title = {{{KiloNeRF}}: {{Speeding}} up {{Neural Radiance Fields}} with {{Thousands}} of {{Tiny MLPs}}},
  shorttitle = {{{KiloNeRF}}},
  author = {Reiser, Christian and Peng, Songyou and Liao, Yiyi and Geiger, Andreas},
  date = {2021-08-02},
  eprint = {2103.13744},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.13744},
  urldate = {2024-05-21},
  abstract = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep MultiLayer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-21T11:44:58.813Z},
  file = {/home/jorge/Zotero/storage/X5FQLPBJ/Reiser et al. - 2021 - KiloNeRF Speeding up Neural Radiance Fields with Thousands of Tiny MLPs.pdf}
}

@inproceedings{tschernezkiNeuralFeatureFusion2022,
  title = {Neural {{Feature Fusion Fields}}: {{3D Distillation}} of {{Self-Supervised 2D Image Representations}}},
  shorttitle = {Neural {{Feature Fusion Fields}}},
  booktitle = {2022 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Tschernezki, Vadim and Laina, Iro and Larlus, Diane and Vedaldi, Andrea},
  date = {2022-09},
  pages = {443--453},
  publisher = {IEEE},
  location = {Prague, Czech Republic},
  doi = {10.1109/3DV57658.2022.00056},
  url = {https://ieeexplore.ieee.org/document/10044452/},
  urldate = {2024-05-22},
  eventtitle = {2022 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  isbn = {978-1-66545-670-8},
  langid = {english},
  keywords = {NeRF,Vision Foundation Models},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-22T11:55:04.148Z},
  file = {/home/jorge/Zotero/storage/FUXN42F8/Tschernezki et al. - 2022 - Neural Feature Fusion Fields 3D Distillation of Self-Supervised 2D Image Representations.pdf}
}

@inproceedings{kerrLERFLanguageEmbedded2023,
  title = {{{LERF}}: {{Language Embedded Radiance Fields}}},
  shorttitle = {{{LERF}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  date = {2023-10-01},
  pages = {19672--19682},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICCV51070.2023.01807},
  url = {https://ieeexplore.ieee.org/document/10376596/},
  urldate = {2024-05-22},
  eventtitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {9798350307184},
  langid = {english},
  keywords = {NeRF,Vision Foundation Models},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-22T12:46:36.107Z},
  file = {/home/jorge/Zotero/storage/WMYBJSQU/Kerr et al. - 2023 - LERF Language Embedded Radiance Fields.pdf}
}

@online{siddiquiPanopticLifting3D2022,
  title = {Panoptic {{Lifting}} for {{3D Scene Understanding}} with {{Neural Fields}}},
  author = {Siddiqui, Yawar and Porzi, Lorenzo and Bul√≥, Samuel Rota and M√ºller, Norman and Nie√üner, Matthias and Dai, Angela and Kontschieder, Peter},
  date = {2022-12-19},
  eprint = {2212.09802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.09802},
  urldate = {2024-05-22},
  abstract = {We propose Panoptic Lifting, a novel approach for learning panoptic 3D volumetric representations from images of in-the-wild scenes. Once trained, our model can render color images together with 3D-consistent panoptic segmentation from novel viewpoints. Unlike existing approaches which use 3D input directly or indirectly, our method requires only machine-generated 2D panoptic segmentation masks inferred from a pre-trained network. Our core contribution is a panoptic lifting scheme based on a neural field representation that generates a unified and multi-view consistent, 3D panoptic representation of the scene. To account for inconsistencies of 2D instance identifiers across views, we solve a linear assignment with a cost based on the model‚Äôs current predictions and the machine-generated segmentation masks, thus enabling us to lift 2D instances to 3D in a consistent way. We further propose and ablate contributions that make our method more robust to noisy, machine-generated labels, including test-time augmentations for confidence estimates, segment consistency loss, bounded segmentation fields, and gradient stopping. Experimental results validate our approach on the challenging Hypersim, Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6\% in scene-level PQ over state of the art.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-22T15:19:55.180Z},
  file = {/home/jorge/Zotero/storage/BCPDBAW3/Siddiqui et al. - 2022 - Panoptic Lifting for 3D Scene Understanding with Neural Fields.pdf}
}

@online{fanNeRFSOSAnyViewSelfsupervised2022,
  title = {{{NeRF-SOS}}: {{Any-View Self-supervised Object Segmentation}} on {{Complex Scenes}}},
  shorttitle = {{{NeRF-SOS}}},
  author = {Fan, Zhiwen and Wang, Peihao and Jiang, Yifan and Gong, Xinyu and Xu, Dejia and Wang, Zhangyang},
  date = {2022-10-11},
  eprint = {2209.08776},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.08776},
  urldate = {2024-05-22},
  abstract = {Neural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation (NeRF-SOS), couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, BlendedMVS, CO3Dv2, and Tank \& Temples datasets validate the effectiveness of NeRF-SOS. It consistently surpasses other 2D-based self-supervised baselines and predicts finer object masks than existing supervised counterparts. Please refer to the video on our project page for more details: https://zhiwenfan.github.io/NeRF-SOS/.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-22T15:23:08.904Z},
  file = {/home/jorge/Zotero/storage/VX33GATR/Fan et al. - 2022 - NeRF-SOS Any-View Self-supervised Object Segmentation on Complex Scenes.pdf}
}

@online{CLIP,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2024-05-23},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  keywords = {üî•,Vision Foundation Models},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T08:52:10.213Z},
  file = {/home/jorge/Zotero/storage/XCQC8VTF/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf}
}

@online{DeiT,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J√©gou, Herv√©},
  date = {2021-01-15},
  eprint = {2012.12877},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.12877},
  urldate = {2024-05-23},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {ViT},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T10:21:46.937Z},
  file = {/home/jorge/Zotero/storage/2QPFS233/Touvron et al. - 2021 - Training data-efficient image transformers & distillation through attention.pdf}
}

@article{chenGNeSFGeneralizableNeural2023,
  title = {{{GNeSF}}: {{Generalizable Neural Semantic Fields}}},
  shorttitle = {{{GNeSF}}},
  author = {Chen, Hanlin and Li, Chen and Guo, Mengqi and Yan, Zhiwen and Lee, Gim Hee},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.15712},
  url = {https://arxiv.org/abs/2310.15712},
  urldate = {2024-05-23},
  abstract = {3D scene segmentation based on neural implicit representation has emerged recently with the advantage of training only on 2D supervision. However, existing approaches still requires expensive per-scene optimization that prohibits generalization to novel scenes during inference. To circumvent this problem, we introduce a generalizable 3D segmentation framework based on implicit representation. Specifically, our framework takes in multi-view image features and semantic maps as the inputs instead of only spatial information to avoid overfitting to scene-specific geometric and semantic information. We propose a novel soft voting mechanism to aggregate the 2D semantic information from different views for each 3D point. In addition to the image features, view difference information is also encoded in our framework to predict the voting scores. Intuitively, this allows the semantic information from nearby views to contribute more compared to distant ones. Furthermore, a visibility module is also designed to detect and filter out detrimental information from occluded views. Due to the generalizability of our proposed method, we can synthesize semantic maps or conduct 3D semantic segmentation for novel scenes with solely 2D semantic supervision. Experimental results show that our approach achieves comparable performance with scene-specific approaches. More importantly, our approach can even outperform existing strong supervision-based approaches with only 2D annotations. Our source code is available at: https://github.com/HLinChen/GNeSF.},
  version = {2},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T11:44:14.361Z},
  file = {/home/jorge/Zotero/storage/2MCQR6KB/Chen et al. - 2023 - GNeSF Generalizable Neural Semantic Fields.pdf}
}

@article{liNeuralSceneFlow2021,
  title = {Neural {{Scene Flow Fields}} for {{Space-Time View Synthesis}} of {{Dynamic Scenes}}},
  author = {Li, Zhengqi and Niklaus, Simon and Snavely, Noah and Wang, Oliver},
  date = {2021-06},
  journaltitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6494--6504},
  publisher = {IEEE},
  location = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.00643},
  url = {https://ieeexplore.ieee.org/document/9578364/},
  urldate = {2024-05-23},
  abstract = {We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for varieties of in-the-wild scenes, including thin structures, view-dependent effects, and complex degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9781665445092},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T12:36:54.294Z},
  file = {/home/jorge/Zotero/storage/ADBTKXUI/Li et al. - 2021 - Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes.pdf}
}

@online{tLift3DZeroShotLifting2024,
  title = {{{Lift3D}}: {{Zero-Shot Lifting}} of {{Any 2D Vision Model}} to {{3D}}},
  shorttitle = {{{Lift3D}}},
  author = {T, Mukund Varma and Wang, Peihao and Fan, Zhiwen and Wang, Zhangyang and Su, Hao and Ramamoorthi, Ravi},
  date = {2024-03-27},
  eprint = {2403.18922},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.18922},
  urldate = {2024-05-23},
  abstract = {In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T19:50:30.557Z},
  file = {/home/jorge/Zotero/storage/MAZBMTDY/T et al. - 2024 - Lift3D Zero-Shot Lifting of Any 2D Vision Model to 3D.pdf}
}

@online{wangAdaptiveShellsEfficient2023,
  title = {Adaptive {{Shells}} for {{Efficient Neural Radiance Field Rendering}}},
  author = {Wang, Zian and Shen, Tianchang and Nimier-David, Merlin and Sharp, Nicholas and Gao, Jun and Keller, Alexander and Fidler, Sanja and M√ºller, Thomas and Gojcic, Zan},
  date = {2023-11-16},
  eprint = {2311.10091},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.10091},
  urldate = {2024-05-23},
  abstract = {Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-23T20:44:04.386Z},
  file = {/home/jorge/Zotero/storage/2422YS6T/Wang et al. - 2023 - Adaptive Shells for Efficient Neural Radiance Field Rendering.pdf}
}

@online{tewariAdvancesNeuralRendering2022,
  title = {Advances in {{Neural Rendering}}},
  author = {Tewari, Ayush and Thies, Justus and Mildenhall, Ben and Srinivasan, Pratul and Tretschk, Edgar and Wang, Yifan and Lassner, Christoph and Sitzmann, Vincent and Martin-Brualla, Ricardo and Lombardi, Stephen and Simon, Tomas and Theobalt, Christian and Niessner, Matthias and Barron, Jonathan T. and Wetzstein, Gordon and Zollhoefer, Michael and Golyanik, Vladislav},
  date = {2022-03-30},
  eprint = {2111.05849},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.05849},
  urldate = {2024-05-24},
  abstract = {Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling nonrigidly deforming objects and scene editing and composition. While most of these approaches are scene-specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state-ofthe-art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-24T14:37:20.060Z},
  file = {/home/jorge/Zotero/storage/LWV5VZGE/Tewari et al. - 2022 - Advances in Neural Rendering.pdf}
}

@online{NeuS,
  title = {{{NeuS}}: {{Learning Neural Implicit Surfaces}} by {{Volume Rendering}} for {{Multi-view Reconstruction}}},
  shorttitle = {{{NeuS}}},
  author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  date = {2023-02-01},
  eprint = {2106.10689},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.10689},
  urldate = {2024-05-24},
  abstract = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR [Niemeyer et al., 2020] and IDR [Yariv et al., 2020], require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF [Mildenhall et al., 2020] and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-24T15:14:51.957Z},
  file = {/home/jorge/Zotero/storage/9RHCZY5U/Wang et al. - 2023 - NeuS Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction.pdf}
}

@online{parkDeepSDFLearningContinuous2019,
  title = {{{DeepSDF}}: {{Learning Continuous Signed Distance Functions}} for {{Shape Representation}}},
  shorttitle = {{{DeepSDF}}},
  author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  date = {2019-01-15},
  eprint = {1901.05103},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1901.05103},
  urldate = {2024-05-25},
  abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape‚Äôs surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape‚Äôs boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF‚Äôs both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show stateof-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-25T13:28:15.920Z},
  file = {/home/jorge/Zotero/storage/LIJCLNPE/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Functions for Shape Representation.pdf}
}

@inproceedings{renNeuralVolumetricObject2022,
  title = {Neural {{Volumetric Object Selection}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ren, Zhongzheng and Agarwala, Aseem and Russell, Bryan and Schwing, Alexander G. and Wang, Oliver},
  date = {2022-06},
  pages = {6123--6132},
  publisher = {IEEE},
  location = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00604},
  url = {https://ieeexplore.ieee.org/document/9879854/},
  urldate = {2024-05-26},
  abstract = {We introduce an approach for selecting objects in neural volumetric 3D representations, such as multi-plane images (MPI) and neural radiance fields (NeRF). Our approach takes a set of foreground and background 2D user scribbles in one view and automatically estimates a 3D segmentation of the desired object, which can be rendered into novel views. To achieve this result, we propose a novel voxel feature embedding that incorporates the neural volumetric 3D representation and multi-view image features from all input views. To evaluate our approach, we introduce a new dataset of human-provided segmentation masks for depicted objects in real-world multi-view scene captures. We show that our approach out-performs strong baselines, including 2D segmentation and 3D segmentation approaches adapted to our task.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-26T11:09:37.392Z},
  file = {/home/jorge/Zotero/storage/Z3YV5VCZ/Ren et al. - 2022 - Neural Volumetric Object Selection.pdf}
}

@online{liuStyleRFZeroshot3D2023,
  title = {{{StyleRF}}: {{Zero-shot 3D Style Transfer}} of {{Neural Radiance Fields}}},
  shorttitle = {{{StyleRF}}},
  author = {Liu, Kunhao and Zhan, Fangneng and Chen, Yiwen and Zhang, Jiahui and Yu, Yingchen and Saddik, Abdulmotaleb El and Lu, Shijian and Xing, Eric},
  date = {2023-03-24},
  eprint = {2303.10598},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.10598},
  urldate = {2024-05-26},
  abstract = {3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-26T12:07:44.932Z},
  file = {/home/jorge/Zotero/storage/X2JZNN86/Liu et al. - 2023 - StyleRF Zero-shot 3D Style Transfer of Neural Radiance Fields.pdf}
}

@inproceedings{wangSemanticEnoughOnly2023,
  title = {Semantic {{Is Enough}}: {{Only Semantic Information For NeRF Reconstruction}}},
  shorttitle = {Semantic {{Is Enough}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Unmanned Systems}} ({{ICUS}})},
  author = {Wang, Ruibo and Zhang, Song and Huang, Ping and Zhang, Donghai and Yan, Wei},
  date = {2023-10-13},
  eprint = {2403.16043},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {906--912},
  doi = {10.1109/ICUS58632.2023.10318339},
  url = {http://arxiv.org/abs/2403.16043},
  urldate = {2024-05-27},
  abstract = {Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels. This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component. We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model‚Äôs semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach. We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model. Our primary objective is to obverse the impact of this modification on the model‚Äôs performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation. The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-27T19:33:43.114Z},
  file = {/home/jorge/Zotero/storage/N7B5X2QY/Wang et al. - 2023 - Semantic Is Enough Only Semantic Information For NeRF Reconstruction.pdf}
}

@online{jainPuttingNeRFDiet2021,
  title = {Putting {{NeRF}} on a {{Diet}}: {{Semantically Consistent Few-Shot View Synthesis}}},
  shorttitle = {Putting {{NeRF}} on a {{Diet}}},
  author = {Jain, Ajay and Tancik, Matthew and Abbeel, Pieter},
  date = {2021-04-01},
  eprint = {2104.00677},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.00677},
  urldate = {2024-05-27},
  abstract = {We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360‚ó¶ scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions. Our project website is available at https://www.ajayj.com/dietnerf.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-27T19:46:13.303Z},
  file = {/home/jorge/Zotero/storage/3FCWDWWI/Jain et al. - 2021 - Putting NeRF on a Diet Semantically Consistent Few-Shot View Synthesis.pdf}
}

@online{chenTensoRFTensorialRadiance2022,
  title = {{{TensoRF}}: {{Tensorial Radiance Fields}}},
  shorttitle = {{{TensoRF}}},
  author = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
  date = {2022-11-29},
  eprint = {2203.09517},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.09517},
  urldate = {2024-05-28},
  abstract = {We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CANDECOMP/PARAFAC (CP) decomposition ‚Äì that factorizes tensors into rank-one components with compact vectors ‚Äì in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction ({$<$} 30 min) with better rendering quality and even a smaller model size ({$<$} 4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time ({$<$} 10 min) and retaining a compact model size ({$<$} 75 MB).},
  langid = {english},
  pubstate = {prepublished},
  keywords = {NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-05-28T13:21:37.382Z},
  file = {/home/jorge/Zotero/storage/S7V4KGKH/Chen et al. - 2022 - TensoRF Tensorial Radiance Fields.pdf}
}

@online{darcetVisionTransformersNeed2024,
  title = {Vision {{Transformers Need Registers}}},
  author = {Darcet, Timoth√©e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  date = {2024-04-12},
  eprint = {2309.16588},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.16588},
  urldate = {2024-06-01},
  abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-01T14:56:33.784Z},
  file = {/home/jorge/Zotero/storage/9DBXKWEB/Darcet et al. - 2024 - Vision Transformers Need Registers.pdf}
}

@online{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll√°r, Piotr and Girshick, Ross},
  date = {2023-04-05},
  eprint = {2304.02643},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.02643},
  urldate = {2024-06-01},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive ‚Äì often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-01T15:29:40.075Z},
  file = {/home/jorge/Zotero/storage/J73DAL4H/Kirillov et al. - 2023 - Segment Anything.pdf}
}

@online{goliBayesRaysUncertainty2023,
  title = {Bayes' {{Rays}}: {{Uncertainty Quantification}} for {{Neural Radiance Fields}}},
  shorttitle = {Bayes' {{Rays}}},
  author = {Goli, Lily and Reading, Cody and Sell√°n, Silvia and Jacobson, Alec and Tagliasacchi, Andrea},
  date = {2023-09-06},
  eprint = {2309.03185},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.03185},
  urldate = {2024-06-02},
  abstract = {Neural Radiance Fields (NeRFs) have shown promise in applications like view synthesis and depth estimation, but learning from multiview images faces inherent uncertainties. Current methods to quantify them are either heuristic or computationally demanding. We introduce BayesRays, a post-hoc framework to evaluate uncertainty in any pre-trained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and show its superior performance in key metrics and applications. Additional results available at: https://bayesrays.github.io.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-02T11:14:59.717Z},
  file = {/home/jorge/Zotero/storage/T36TSZVG/Goli et al. - 2023 - Bayes' Rays Uncertainty Quantification for Neural Radiance Fields.pdf}
}

@article{princeUnderstandingDeepLearning,
  title = {Understanding {{Deep Learning}}},
  author = {Prince, Simon J D},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-02T11:41:46.114Z},
  file = {/home/jorge/Zotero/storage/HCW9WJ4H/Prince - Understanding Deep Learning.pdf}
}

@article{cabiscolUnderstandingUncertaintyBayesian,
  title = {Understanding {{Uncertainty}} in {{Bayesian Neural Networks}}},
  author = {Cabiscol, Javier Antor√°n},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-02T13:43:13.306Z},
  file = {/home/jorge/Zotero/storage/FJ6ZYUDR/Cabiscol - Understanding Uncertainty in Bayesian Neural Networks.pdf}
}

@article{ritterSCALABLELAPLACEAPPROXIMATION2018,
  title = {A {{SCALABLE LAPLACE APPROXIMATION FOR NEURAL NETWORKS}}},
  author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  date = {2018},
  abstract = {We leverage recent insights from second-order optimisation for neural networks to construct a Kronecker factored Laplace approximation to the posterior over the weights of a trained network. Our approximation requires no modification of the training procedure, enabling practitioners to estimate the uncertainty of their models currently used in production without having to retrain them. We extensively compare our method to using Dropout and a diagonal Laplace approximation for estimating the uncertainty of a network. We demonstrate that our Kronecker factored method leads to better uncertainty estimates on out-of-distribution data and is more robust to simple adversarial attacks. Our approach only requires calculating two square curvature factor matrices for each layer. Their size is equal to the respective square of the input and output size of the layer, making the method efficient both computationally and in terms of memory usage. We illustrate its scalability by applying it to a state-of-the-art convolutional network architecture.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-02T20:54:49.000Z},
  file = {/home/jorge/Zotero/storage/XG5SSPUX/Ritter et al. - 2018 - A SCALABLE LAPLACE APPROXIMATION FOR NEURAL NETWORKS.pdf}
}

@article{kendallWhatUncertaintiesWe,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model ‚Äì uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-03T14:22:57.303Z},
  file = {/home/jorge/Zotero/storage/AB4LG84Z/Kendall and Gal - What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf}
}

@inproceedings{caoTexFusionSynthesizing3D2023,
  title = {{{TexFusion}}: {{Synthesizing 3D Textures}} with {{Text-Guided Image Diffusion Models}}},
  shorttitle = {{{TexFusion}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Cao, Tianshi and Kreis, Karsten and Fidler, Sanja and Sharp, Nicholas and Yin, Kangxue},
  date = {2023-10-01},
  pages = {4146--4158},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00385},
  url = {https://ieeexplore.ieee.org/document/10377331/},
  urldate = {2024-06-08},
  eventtitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {9798350307184},
  langid = {english},
  keywords = {3D,Diffusion,Texturing},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-08T19:09:52.986Z},
  file = {/home/jorge/Zotero/storage/42N52F8R/Cao et al. - 2023 - TexFusion Synthesizing 3D Textures with Text-Guided Image Diffusion Models.pdf}
}

@article{devoreModernIntroductionProbability2006,
  title = {A {{Modern Introduction}} to {{Probability}} and {{Statistics}}: {{Understanding Why}} and {{How}}},
  shorttitle = {A {{Modern Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Devore, Jay},
  date = {2006-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {101},
  number = {473},
  pages = {393--394},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2006.s72},
  url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2006.s72},
  urldate = {2024-06-08},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-08T19:16:21.100Z},
  file = {/home/jorge/Zotero/storage/W7SE5JHN/Devore - 2006 - A Modern Introduction to Probability and Statistics Understanding Why and How.pdf}
}

@article{caiGenerativeRenderingControllable,
  title = {Generative {{Rendering}}: {{Controllable 4D-Guided Video Generation}} with {{2D Diffusion Models}}},
  author = {Cai, Shengqu and Ceylan, Duygu and Gadelha, Matheus and Huang, Chun-Hao Paul and Wang, Tuanfeng Yang and Wetzstein, Gordon},
  abstract = {Traditional 3D content creation tools empower users to bring their imagination to life by giving them direct control over a scene‚Äôs geometry, appearance, motion, and camera path. Creating computer-generated videos, however, is a tedious manual process, which can be automated by emerging text-to-video diffusion models. Despite great promise, video diffusion models are difficult to control, hindering a user to apply their own creativity rather than amplifying it. To address this challenge, we present a novel approach that combines the controllability of dynamic 3D meshes with the expressivity and editability of emerging diffusion models. For this purpose, our approach takes an animated, low-fidelity rendered mesh as input and injects the ground truth correspondence information obtained from the dynamic mesh into various stages of a pre-trained text-to-image generation model to output high-quality and temporally consistent frames. We demonstrate our approach on various examples where motion can be obtained by animating rigged assets or changing the camera path. Project page: primecai.github.io/ generative rendering.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-09T19:25:07.062Z},
  file = {/home/jorge/Zotero/storage/FGJIGDZA/Cai et al. - Generative Rendering Controllable 4D-Guided Video Generation with 2D Diffusion Models.pdf}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-06-09},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-09T19:52:59.832Z},
  file = {/home/jorge/Zotero/storage/V5NA2DMQ/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@online{duttDiffusion3DFeatures2024,
  title = {Diffusion {{3D Features}} ({{Diff3F}}): {{Decorating Untextured Shapes}} with {{Distilled Semantic Features}}},
  shorttitle = {Diffusion {{3D Features}} ({{Diff3F}})},
  author = {Dutt, Niladri Shekhar and Muralikrishnan, Sanjeev and Mitra, Niloy J.},
  date = {2024-04-02},
  eprint = {2311.17024},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17024},
  urldate = {2024-06-10},
  abstract = {We present Diff3F as a simple, robust, and class-agnostic feature descriptor that can be computed for untextured input shapes (meshes or point clouds). Our method distills diffusion features from image foundational models onto input shapes. Specifically, we use the input shapes to produce depth and normal maps as guidance for conditional image synthesis. In the process, we produce (diffusion) features in 2D that we subsequently lift and aggregate on the original surface. Our key observation is that even if the conditional image generations obtained from multi-view rendering of the input shapes are inconsistent, the associated image features are robust and, hence, can be directly aggregated across views. This produces semantic features on the input shapes, without requiring additional data or training. We perform extensive experiments on multiple benchmarks (SHREC'19, SHREC'20, FAUST, and TOSCA) and demonstrate that our features, being semantic instead of geometric, produce reliable correspondence across both isometric and non-isometrically related shape families. Code is available via the project page at https://diff3f.github.io/},
  langid = {english},
  pubstate = {prepublished},
  keywords = {3D,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-10T14:43:19.442Z},
  file = {/home/jorge/Zotero/storage/8U6MHJVK/Dutt et al. - 2024 - Diffusion 3D Features (Diff3F) Decorating Untextured Shapes with Distilled Semantic Features.pdf}
}

@online{girshickRichFeatureHierarchies2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-10-22},
  eprint = {1311.2524},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1311.2524},
  urldate = {2024-06-10},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012‚Äîachieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/Àúrbg/rcnn.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Object Detection},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-10T20:07:12.668Z},
  file = {/home/jorge/Zotero/storage/WR288AYZ/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf}
}

@online{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  date = {2015-09-27},
  eprint = {1504.08083},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1504.08083},
  urldate = {2024-06-10},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9√ó faster than R-CNN, is 213√ó faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3√ó faster, tests 10√ó faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Object Detection},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-10T20:55:24.862Z},
  file = {/home/jorge/Zotero/storage/P4VC72H4/Girshick - 2015 - Fast R-CNN.pdf}
}

@online{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2024-06-10},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Object Detection},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-10T21:20:28.037Z},
  file = {/home/jorge/Zotero/storage/ZBQ99YRP/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf}
}

@online{DDIM,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  date = {2022-10-05},
  eprint = {2010.02502},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.02502},
  urldate = {2024-06-16},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10√ó to 50√ó faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T12:31:56.350Z},
  file = {/home/jorge/Zotero/storage/IR94D3CB/Song et al. - 2022 - Denoising Diffusion Implicit Models.pdf}
}

@online{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-11-18},
  eprint = {1503.03585},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, q-bio, stat},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2024-06-16},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T12:38:18.845Z},
  file = {/home/jorge/Zotero/storage/N3YHXPF3/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf}
}

@online{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  date = {2021-02-18},
  eprint = {2102.09672},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2102.09672},
  urldate = {2024-06-16},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive loglikelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/ openai/improved-diffusion.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T12:41:34.798Z},
  file = {/home/jorge/Zotero/storage/LPHRIAUC/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf}
}

@online{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  date = {2021-06-01},
  eprint = {2105.05233},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2105.05233},
  urldate = {2024-06-16},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128√ó128, 4.59 on ImageNet 256√ó256, and 7.72 on ImageNet 512√ó512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256√ó256 and 3.85 on ImageNet 512√ó512. We release our code at https://github.com/openai/guided-diffusion.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T12:43:29.467Z},
  file = {/home/jorge/Zotero/storage/YS3VSFN4/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf}
}

@online{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj√∂rn},
  date = {2022-04-13},
  eprint = {2112.10752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2024-06-16},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T13:26:23.272Z},
  file = {/home/jorge/Zotero/storage/N6CPFUHD/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf;/home/jorge/Zotero/storage/XYVBD7NA/2309.16653v2.pdf}
}

@online{metzerLatentNeRFShapeGuidedGeneration2022,
  title = {Latent-{{NeRF}} for {{Shape-Guided Generation}} of {{3D Shapes}} and {{Textures}}},
  author = {Metzer, Gal and Richardson, Elad and Patashnik, Or and Giryes, Raja and Cohen-Or, Daniel},
  date = {2022-11-14},
  eprint = {2211.07600},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.07600},
  urldate = {2024-06-16},
  abstract = {Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a NeRF model to generate a 3D object. We adapt the score distillation to the publicly available, and computationally efficient, Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoencoder. As NeRFs operate in image space, a na¬®ƒ±ve solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the NeRF to the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we show that while Text-to-3D models can generate impressive results, they are inherently unconstrained and may lack the ability to guide or enforce a specific 3D structure. To assist and direct the 3D generation, we propose to guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-NeRF. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry. Our experiments validate the power of our different forms of guidance and the efficiency of using latent rendering.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Diffusion,Texturing},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T14:15:29.709Z},
  file = {/home/jorge/Zotero/storage/WHPQVHUJ/Metzer et al. - 2022 - Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures.pdf}
}

@online{richardsonTEXTureTextGuidedTexturing2023,
  title = {{{TEXTure}}: {{Text-Guided Texturing}} of {{3D Shapes}}},
  shorttitle = {{{TEXTure}}},
  author = {Richardson, Elad and Metzer, Gal and Alaluf, Yuval and Giryes, Raja and Cohen-Or, Daniel},
  date = {2023-02-03},
  eprint = {2302.01721},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.01721},
  urldate = {2024-06-16},
  abstract = {In this paper, we present TEXTure, a novel method for text-guided generation, editing, and transfer of textures for 3D shapes. Leveraging a pretrained depth-to-image diffusion model, TEXTure applies an iterative scheme that paints a 3D model from different viewpoints. Yet, while depth-to-image models can create plausible textures from a single viewpoint, the stochastic nature of the generation process can cause many inconsistencies when texturing an entire 3D object. To tackle these problems, we dynamically define a trimap partitioning of the rendered image into three progression states, and present a novel elaborated diffusion sampling process that uses this trimap representation to generate seamless textures from different views. We then show that one can transfer the generated texture maps to new 3D geometries without requiring explicit surface-to-surface mapping, as well as extract semantic textures from a set of images without requiring any explicit reconstruction. Finally, we show that TEXTure can be used to not only generate new textures but also edit and refine existing textures using either a text prompt or user-provided scribbles. We demonstrate that our TEXTuring method excels at generating, transferring, and editing textures through extensive evaluation, and further close the gap between 2D image generation and 3D texturing.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {3D,Diffusion,Texturing},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-16T14:15:33.642Z},
  file = {/home/jorge/Zotero/storage/FNLXCXT9/Richardson et al. - 2023 - TEXTure Text-Guided Texturing of 3D Shapes.pdf}
}

@online{pooleDreamFusionTextto3DUsing2022,
  title = {{{DreamFusion}}: {{Text-to-3D}} Using {{2D Diffusion}}},
  shorttitle = {{{DreamFusion}}},
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  date = {2022-09-29},
  eprint = {2209.14988},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2209.14988},
  urldate = {2024-06-18},
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors. See dreamfusion3d.github.io for a more immersive view into our 3D results.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {üî•,3D,Diffusion,NeRF},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-18T12:53:03.428Z},
  file = {/home/jorge/Zotero/storage/CLLCWJYM/Poole et al. - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf}
}

@online{uzolasMotionDreamerZeroShot3D2024,
  title = {{{MotionDreamer}}: {{Zero-Shot 3D Mesh Animation}} from {{Video Diffusion Models}}},
  shorttitle = {{{MotionDreamer}}},
  author = {Uzolas, Lukas and Eisemann, Elmar and Kellnhofer, Petr},
  date = {2024-05-30},
  eprint = {2405.20155},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.20155},
  urldate = {2024-06-19},
  abstract = {Animation techniques bring digital 3D worlds and characters to life. However, manual animation is tedious and automated techniques are often specialized to narrow shape classes. In our work, we propose a technique for automatic reanimation of arbitrary 3D shapes based on a motion prior extracted from a video diffusion model. Unlike existing 4D generation methods, we focus solely on the motion, and we leverage an explicit mesh-based representation compatible with existing computer-graphics pipelines. Furthermore, our utilization of diffusion features enhances accuracy of our motion fitting. We analyze efficacy of these features for animation fitting and we experimentally validate our approach for two different diffusion models and four animation models. Finally, we demonstrate that our time-efficient zero-shot method achieves a superior performance re-animating a diverse set of 3D shapes when compared to existing techniques in a user study. The project website is located at https://lukas.uzolas.com/MotionDreamer.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-19T11:48:11.693Z},
  file = {/home/jorge/Zotero/storage/UVF97FCC/Uzolas et al. - 2024 - MotionDreamer Zero-Shot 3D Mesh Animation from Video Diffusion Models.pdf}
}

@online{luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  date = {2022-08-25},
  eprint = {2208.11970},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2208.11970},
  urldate = {2024-06-22},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-22T22:13:09.385Z},
  file = {/home/jorge/Zotero/storage/NDPLLNGS/Luo - 2022 - Understanding Diffusion Models A Unified Perspective.pdf}
}

@inproceedings{zhangDiffCollageParallelGeneration2023,
  title = {{{DiffCollage}}: {{Parallel Generation}} of {{Large Content}} with {{Diffusion Models}}},
  shorttitle = {{{DiffCollage}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Qinsheng and Song, Jiaming and Huang, Xun and Chen, Yongxin and Liu, Ming-Yu},
  date = {2023-06},
  pages = {10188--10198},
  publisher = {IEEE},
  location = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00982},
  url = {https://ieeexplore.ieee.org/document/10203861/},
  urldate = {2024-06-24},
  abstract = {We present DiffCollage, a compositional diffusion model that can generate large content by leveraging diffusion models trained on generating pieces of the large content. Our approach is based on a factor graph representation where each factor node represents a portion of the content and a variable node represents their overlap. This representation allows us to aggregate intermediate outputs from diffusion models defined on individual nodes to generate content of arbitrary size and shape in parallel without resorting to an autoregressive generation procedure. We apply DiffCollage to various tasks, including infinite image generation, panorama image generation, and long-duration text-guided motion generation. Extensive experimental results with a comparison to strong autoregressive baselines verify the effectiveness of our approach.},
  eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9798350301298},
  langid = {english},
  keywords = {Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-24T14:09:28.198Z},
  file = {/home/jorge/Zotero/storage/9H2LXIRK/Zhang et al. - 2023 - DiffCollage Parallel Generation of Large Content with Diffusion Models.pdf}
}

@online{ceylanPix2VideoVideoEditing2023,
  title = {{{Pix2Video}}: {{Video Editing}} Using {{Image Diffusion}}},
  shorttitle = {{{Pix2Video}}},
  author = {Ceylan, Duygu and Huang, Chun-Hao Paul and Mitra, Niloy J.},
  date = {2023-03-22},
  eprint = {2303.12688},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.12688},
  urldate = {2024-06-25},
  abstract = {Image diffusion models, trained on massive image collections, have emerged as the most versatile image generator model in terms of quality and diversity. They support inverting real images and conditional (e.g., text) generation, making them attractive for high-quality image editing applications. We investigate how to use such pre-trained image models for text-guided video editing. The critical challenge is to achieve the target edits while still preserving the content of the source video. Our method works in two simple steps: first, we use a pre-trained structure-guided (e.g., depth) image diffusion model to perform text-guided edits on an anchor frame; then, in the key step, we progressively propagate the changes to the future frames via self-attention feature injection to adapt the core denoising step of the diffusion model. We then consolidate the changes by adjusting the latent code for the frame before continuing the process. Our approach is training-free and generalizes to a wide range of edits. We demonstrate the effectiveness of the approach by extensive experimentation and compare it against four different prior and parallel efforts (on ArXiv). We demonstrate that realistic text-guided video edits are possible, without any compute-intensive preprocessing or video-specific finetuning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-25T08:41:38.697Z},
  file = {/home/jorge/Zotero/storage/VWCW7E93/Ceylan et al. - 2023 - Pix2Video Video Editing using Image Diffusion.pdf}
}

@article{geyerTOKENFLOWCONSISTENTDIFFUSION,
  title = {{{TOKENFLOW}}: {{CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING}}},
  author = {Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
  abstract = {The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-toimage editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos.},
  langid = {english},
  keywords = {Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-25T08:44:27.122Z},
  file = {/home/jorge/Zotero/storage/2CU379EI/Geyer et al. - TOKENFLOW CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING.pdf}
}

@online{wuTuneAVideoOneShotTuning2023,
  title = {Tune-{{A-Video}}: {{One-Shot Tuning}} of {{Image Diffusion Models}} for {{Text-to-Video Generation}}},
  shorttitle = {Tune-{{A-Video}}},
  author = {Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  date = {2023-03-17},
  eprint = {2212.11565},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.11565},
  urldate = {2024-06-26},
  abstract = {To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting\$\textbackslash unicode\{x2014\}\$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-26T11:49:57.634Z},
  file = {/home/jorge/Zotero/storage/WJ9JV8GM/Wu et al. - 2023 - Tune-A-Video One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation.pdf}
}

@article{bowersRayTracingApproach2011,
  title = {A {{Ray Tracing Approach}} to {{Diffusion Curves}}},
  author = {Bowers, John C. and Leahey, Jonathan and Wang, Rui},
  date = {2011-06},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {30},
  number = {4},
  pages = {1345--1352},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2011.01994.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.01994.x},
  urldate = {2024-06-27},
  abstract = {Diffusion curves [OBW‚àó08] provide a flexible tool to create smooth-shaded images from curves defined with colors. The resulting image is typically computed by solving a Poisson equation that diffuses the curve colors to the interior of the image. In this paper we present a new method for solving diffusion curves by using ray tracing. Our approach is analogous to final gathering in global illumination, where the curves define source radiance whose visible contribution will be integrated at a shading pixel to produce a color using stochastic ray tracing. Compared to previous work, the main benefit of our method is that it provides artists with extended flexibility in achieving desired image effects. Specifically, we introduce generalized curve colors called shaders that allow for the seamless integration of diffusion curves with classic 2D graphics including vector graphics (e.g. gradient fills) and raster graphics (e.g. patterns and textures). We also introduce several extended curve attributes to customize the contribution of each curve. In addition, our method allows any pixel in the image to be independently evaluated, without having to solve the entire image globally (as required by a Poisson-based approach). Finally, we present a GPU-based implementation that generates solution images at interactive rates, enabling dynamic curve editing. Results show that our method can easily produce a variety of desirable image effects.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-27T08:31:37.279Z},
  file = {/home/jorge/Zotero/storage/3BJ2S8XI/Bowers et al. - 2011 - A Ray Tracing Approach to Diffusion Curves.pdf}
}

@online{wangBreathingNewLife2023,
  title = {Breathing {{New Life}} into {{3D Assets}} with {{Generative Repainting}}},
  author = {Wang, Tianfu and Kanakis, Menelaos and Schindler, Konrad and Van Gool, Luc and Obukhov, Anton},
  date = {2023-10-18},
  eprint = {2309.08523},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.08523},
  urldate = {2024-06-27},
  abstract = {Diffusion-based text-to-image models ignited immense attention from the vision community, artists, and content creators. Broad adoption of these models is due to significant improvement in the quality of generations and efficient conditioning on various modalities, not just text. However, lifting the rich generative priors of these 2D models into 3D is challenging. Recent works have proposed various pipelines powered by the entanglement of diffusion models and neural fields. We explore the power of pretrained 2D diffusion models and standard 3D neural radiance fields as independent, standalone tools and demonstrate their ability to work together in a non-learned fashion. Such modularity has the intrinsic advantage of eased partial upgrades, which became an important property in such a fast-paced domain. Our pipeline accepts any legacy renderable geometry, such as textured or untextured meshes, orchestrates the interaction between 2D generative refinement and 3D consistency enforcement tools, and outputs a painted input geometry in several formats. We conduct a large-scale study on a wide range of objects and categories from the ShapeNetSem dataset and demonstrate the advantages of our approach, both qualitatively and quantitatively.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-27T10:33:01.369Z},
  file = {/home/jorge/Zotero/storage/3V4EZQRP/Wang et al. - 2023 - Breathing New Life into 3D Assets with Generative Repainting.pdf}
}

@article{poStateArtDiffusion2024,
  title = {State of the {{Art}} on {{Diffusion Models}} for {{Visual Computing}}},
  author = {Po, R. and Yifan, W. and Golyanik, V. and Aberman, K. and Barron, J. T. and Bermano, A. and Chan, E. and Dekel, T. and Holynski, A. and Kanazawa, A. and Liu, C.K. and Liu, L. and Mildenhall, B. and Nie√üner, M. and Ommer, B. and Theobalt, C. and Wonka, P. and Wetzstein, G.},
  date = {2024-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {43},
  number = {2},
  pages = {e15063},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.15063},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.15063},
  urldate = {2024-06-27},
  abstract = {The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-06-27T21:51:54.364Z},
  file = {/home/jorge/Zotero/storage/PFPN94YI/Po et al. - 2024 - State of the Art on Diffusion Models for Visual Computing.pdf}
}

@article{mordvintsevDifferentiableImageParameterizations2018,
  title = {Differentiable {{Image Parameterizations}}},
  author = {Mordvintsev, Alexander and Pezzotti, Nicola and Schubert, Ludwig and Olah, Chris},
  date = {2018-07-25},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {3},
  number = {7},
  pages = {e12},
  issn = {2476-0757},
  doi = {10.23915/distill.00012},
  url = {https://distill.pub/2018/differentiable-parameterizations},
  urldate = {2024-07-24},
  abstract = {A powerful, under-explored tool for neural network visualizations and art.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-07-24T14:35:58.179Z},
  file = {/home/jorge/Zotero/storage/LFVNDLL7/differentiable-parameterizations.html}
}

@online{songGenerativeModelingEstimating2020,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2020-10-10},
  eprint = {1907.05600},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.05600},
  urldate = {2024-07-26},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Generative Modelling,Score-based Generative Models},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-07-26T15:50:13.096Z},
  file = {/home/jorge/Zotero/storage/IQPGTPBF/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the Data Distribution.pdf}
}

@online{songImprovedTechniquesTraining2020,
  title = {Improved {{Techniques}} for {{Training Score-Based Generative Models}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2020-10-23},
  eprint = {2006.09011},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.09011},
  urldate = {2024-07-26},
  abstract = {Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32 √ó 32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score-based models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can scale scorebased generative models to various image datasets, with diverse resolutions ranging from 64 √ó 64 to 256 √ó 256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and several LSUN categories.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-07-26T16:07:20.470Z},
  file = {/home/jorge/Zotero/storage/D9KYUU4B/Song and Ermon - 2020 - Improved Techniques for Training Score-Based Generative Models.pdf}
}

@online{linMagic3DHighResolutionTextto3D2023,
  title = {{{Magic3D}}: {{High-Resolution Text-to-3D Content Creation}}},
  shorttitle = {{{Magic3D}}},
  author = {Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  date = {2023-03-25},
  eprint = {2211.10440},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.10440},
  urldate = {2024-08-14},
  abstract = {DreamFusion [33] has recently demonstrated the utility of a pre-trained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF) [25], achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2√ó faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7\% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {3D,Diffusion},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-14T16:05:26.915Z},
  file = {/home/jorge/Zotero/storage/HKPHF6MS/Lin et al. - 2023 - Magic3D High-Resolution Text-to-3D Content Creation.pdf}
}

@online{tangDreamGaussianGenerativeGaussian2024,
  title = {{{DreamGaussian}}: {{Generative Gaussian Splatting}} for {{Efficient 3D Content Creation}}},
  shorttitle = {{{DreamGaussian}}},
  author = {Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},
  date = {2024-03-29},
  eprint = {2309.16653},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.16653},
  urldate = {2024-08-14},
  abstract = {Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-14T16:56:55.758Z},
  file = {/home/jorge/Zotero/storage/KDIMJV8X/Tang et al. - 2024 - DreamGaussian Generative Gaussian Splatting for Efficient 3D Content Creation.pdf}
}

@online{jainZeroShotTextGuidedObject2022,
  title = {Zero-{{Shot Text-Guided Object Generation}} with {{Dream Fields}}},
  author = {Jain, Ajay and Mildenhall, Ben and Barron, Jonathan T. and Abbeel, Pieter and Poole, Ben},
  date = {2022-05-04},
  eprint = {2112.01455},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.01455},
  urldate = {2024-08-15},
  abstract = {We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve fidelity and visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-15T08:32:35.988Z},
  file = {/home/jorge/Zotero/storage/2YVQB57P/Jain et al. - 2022 - Zero-Shot Text-Guided Object Generation with Dream Fields.pdf}
}

@inproceedings{chenText2TexTextdrivenTexture2023,
  title = {{{Text2Tex}}: {{Text-driven Texture Synthesis}} via {{Diffusion Models}}},
  shorttitle = {{{Text2Tex}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Chen, Dave Zhenyu and Siddiqui, Yawar and Lee, Hsin-Ying and Tulyakov, Sergey and Nie√üner, Matthias},
  date = {2023-10-01},
  pages = {18512--18522},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICCV51070.2023.01701},
  url = {https://ieeexplore.ieee.org/document/10377726/},
  urldate = {2024-08-16},
  abstract = {We present Text2Tex, a novel method for generating highquality textures for 3D meshes from the given text prompts. Our method incorporates inpainting into a pre-trained depth-aware image diffusion model to progressively synthesize high resolution partial textures from multiple viewpoints. To avoid accumulating inconsistent and stretched artifacts across views, we dynamically segment the rendered view into a generation mask, which represents the generation status of each visible texel. This partitioned view representation guides the depth-aware inpainting model to generate and update partial textures for the corresponding regions. Furthermore, we propose an automatic view sequence generation scheme to determine the next best view for updating the partial texture. Extensive experiments demonstrate that our method significantly outperforms the existing text-driven approaches and GAN-based methods.},
  eventtitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {9798350307184},
  langid = {english},
  keywords = {3D,Diffusion,Texturing},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-16T09:17:20.302Z},
  file = {/home/jorge/Zotero/storage/ZELSQ8EB/Chen et al. - 2023 - Text2Tex Text-driven Texture Synthesis via Diffusion Models.pdf}
}

@online{huoTexGenTextGuided3D2024,
  title = {{{TexGen}}: {{Text-Guided 3D Texture Generation}} with {{Multi-view Sampling}} and {{Resampling}}},
  shorttitle = {{{TexGen}}},
  author = {Huo, Dong and Guo, Zixin and Zuo, Xinxin and Shi, Zhihao and Lu, Juwei and Dai, Peng and Xu, Songcen and Cheng, Li and Yang, Yee-Hong},
  date = {2024-08-02},
  eprint = {2408.01291},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2408.01291},
  urldate = {2024-08-16},
  abstract = {Given a 3D mesh, we aim to synthesize 3D textures that correspond to arbitrary textual descriptions. Current methods for generating and assembling textures from sampled views often result in prominent seams or excessive smoothing. To tackle these issues, we present TexGen, a novel multi-view sampling and resampling framework for texture generation leveraging a pre-trained text-to-image diffusion model. For view consistent sampling, first of all we maintain a texture map in RGB space that is parameterized by the denoising step and updated after each sampling step of the diffusion model to progressively reduce the view discrepancy. An attention-guided multi-view sampling strategy is exploited to broadcast the appearance information across views. To preserve texture details, we develop a noise resampling technique that aids in the estimation of noise, generating inputs for subsequent denoising steps, as directed by the text prompt and current texture map. Through an extensive amount of qualitative and quantitative evaluations, we demonstrate that our proposed method produces significantly better texture quality for diverse 3D objects with a high degree of view consistency and rich appearance details, outperforming current state-of-the-art methods. Furthermore, our proposed texture generation technique can also be applied to texture editing while preserving the original identity. More experimental results are available at https://dong-huo.github.io/TexGen/},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-16T12:43:29.391Z},
  file = {/home/jorge/Zotero/storage/BCMT824F/Huo et al. - 2024 - TexGen Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling.pdf}
}

@online{leeDreamFlowHighQualityTextto3D2024,
  title = {{{DreamFlow}}: {{High-Quality Text-to-3D Generation}} by {{Approximating Probability Flow}}},
  shorttitle = {{{DreamFlow}}},
  author = {Lee, Kyungmin and Sohn, Kihyuk and Shin, Jinwoo},
  date = {2024-03-22},
  eprint = {2403.14966},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.14966},
  urldate = {2024-08-18},
  abstract = {Recent progress in text-to-3D generation has been achieved through the utilization of score distillation methods: they make use of the pre-trained text-to-image (T2I) diffusion models by distilling via the diffusion model training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-3D optimization by leveraging the T2I diffusion prior in the generative sampling process with a predetermined timestep schedule. To this end, we interpret text-to3D optimization as a multi-view image-to-image translation problem, and propose a solution by approximating the probability flow. By leveraging the proposed novel optimization algorithm, we design DreamFlow, a practical three-stage coarseto-fine text-to-3D optimization framework that enables fast generation of highquality and high-resolution (i.e., 1024x1024) 3D contents. For example, we demonstrate that DreamFlow is 5 times faster than the existing state-of-the-art text-to-3D method, while producing more photorealistic 3D contents. Visit our project page (https://kyungmnlee.github.io/dreamflow.github.io/) for visualizations.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-08-18T17:22:56.191Z},
  file = {/home/jorge/Zotero/storage/SPUKQM7T/Lee et al. - 2024 - DreamFlow High-Quality Text-to-3D Generation by Approximating Probability Flow.pdf}
}
